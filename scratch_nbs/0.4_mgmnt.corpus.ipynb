{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Representation\n",
    "\n",
    "> This module comprises corpus representation from a file or Mongo DB\n",
    ">This is an adapted version of Daniel McCrystal June 2018\n",
    "\n",
    ">Author: @danaderp March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import unittest\n",
    "import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq cisco/CSB-CICDPipelineEdition-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"\n",
    "    Represents a set of source artifacts, a set of target artifacts,\n",
    "    and a ground truth model that represents the links between them.\n",
    "    NOTE: This assumes that artifacts separated by line in the same file begin with\n",
    "    an ID. Thus, the first word of every line will not be considered part of the artifact\n",
    "    for semantic purposes\n",
    "    Attributes:\n",
    "        _name (str): The identifier of the corpus dataset\n",
    "        _corpus_root (str): Path to the root directory of the corpus, from\n",
    "            which the source, target, and truth files will be derived\n",
    "        _sources (list of str): one str per artifact\n",
    "        _targets (list of str): one str per artifact\n",
    "        _truth (dict of str:(dict of str:int)): Holds the truth values of links\n",
    "            between sources and targets. _truth[source][target] == 1 if link exists,\n",
    "            0 otherwise.\n",
    "        _source_index (list of str): aligned with source so that source_index[i]\n",
    "            contains the filename or identifier for source[i]\n",
    "        _target_index (list of str): aligned with target so that\n",
    "            target_index[i] contains the filename or identifier for target[i]\n",
    "        _filetype_whitelist (list of str): See __init__ documentation\n",
    "        _filetype_blacklist(list of str): See __init__ documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        corpus_root=\"\",\n",
    "        source_path=\"requirements\",\n",
    "        target_path=\"source_code\",\n",
    "        stop_words_path=None,\n",
    "        truth_path=None,\n",
    "        execution_traces=None,\n",
    "        corpus_code=None,\n",
    "        languages=[\"english\"],\n",
    "        filetype_whitelist=None,\n",
    "        filetype_blacklist=None,\n",
    "        blank=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name (str): The identifier of the corpus dataset\n",
    "            corpus_root (str): Path to the root directory of the corpus, from\n",
    "                which the source, target, and truth files will be derived\n",
    "            source_path (str): Path from corpus root to file or directory of\n",
    "                source corpus. If directory, artifacts will be parsed from\n",
    "                files in the directory. If file, artifacts will be parsed from\n",
    "                lines in the file.\n",
    "            target_path (str): Path from corpus root to file or directory of\n",
    "                target corpus. If directory, artifacts will be parsed from\n",
    "                files in the directory. If file, artifacts will be parsed from\n",
    "                lines in the file.\n",
    "            truth_path (str): Path to file containing ground truth\n",
    "            natural_language (str or list of str, optional): The human language(s) that artifacts\n",
    "                are written in. Will be matched to the corresponding stop words\n",
    "                list and stemmer.\n",
    "            code_language (str or list of str, optional): The programming language(s) that artifacts\n",
    "                are written in. Will be matched to the corresponding stop words list.\n",
    "            filetype_whitelist (list of str, optional): List of file extensions\n",
    "                which will be converted into artifacts. If None (default), all\n",
    "                file extensions will be converted. Do not include dot.\n",
    "            filetype_blacklist(list of str, optional): List of file extensions\n",
    "                which will be ignored when converting into artifacts. If None\n",
    "                (default), all file extensions will be converted. Do not include\n",
    "                dot.\n",
    "        \"\"\"\n",
    "        self._name = name\n",
    "\n",
    "        if blank:\n",
    "            return\n",
    "\n",
    "        self._corpus_root = corpus_root\n",
    "\n",
    "        self._filetype_whitelist = filetype_whitelist\n",
    "        self._filetype_blacklist = filetype_blacklist\n",
    "\n",
    "        self._source_index = []\n",
    "        self._target_index = []\n",
    "\n",
    "        if type(source_path) is not list:\n",
    "            source_path = [source_path]\n",
    "\n",
    "        if type(target_path) is not list:\n",
    "            target_path = [target_path]\n",
    "\n",
    "        self._sources = []\n",
    "        for sp in source_path:\n",
    "            self._sources += self._parse_artifacts(sp, self._source_index)\n",
    "\n",
    "        self._targets = []\n",
    "        for tp in target_path:\n",
    "            self._targets += self._parse_artifacts(tp, self._target_index)\n",
    "\n",
    "        self._truth = None\n",
    "        if truth_path is not None:\n",
    "            self._truth = dict()\n",
    "            self._all_links = []\n",
    "\n",
    "            for source in self._source_index:\n",
    "                self._truth[source] = dict()\n",
    "                for target in self._target_index:\n",
    "                    self._truth[source][target] = 0\n",
    "                    self._all_links.append((source, target))\n",
    "\n",
    "            if type(truth_path) is not list:\n",
    "                truth_path = [truth_path]\n",
    "\n",
    "            for tp in truth_path:\n",
    "                with open(corpus_root + tp, \"r\") as truth_file:\n",
    "                    for line in truth_file.readlines():\n",
    "                        tokens = line.split()\n",
    "\n",
    "                        source = tokens[0]\n",
    "\n",
    "                        if source not in self._truth:\n",
    "                            raise KeyError(\n",
    "                                \"Source artifact '\"\n",
    "                                + source\n",
    "                                + \"' in truth file not a recognized artifact\"\n",
    "                            )\n",
    "\n",
    "                        for target in tokens[1:]:\n",
    "                            if target not in self._truth[source]:\n",
    "                                print(source)\n",
    "                                print(self._truth[source])\n",
    "                                raise KeyError(\n",
    "                                    \"Target artifact '\"\n",
    "                                    + target\n",
    "                                    + \"' in truth file not a recognized artifact\"\n",
    "                                )\n",
    "\n",
    "                            self._truth[source][target] = 1\n",
    "\n",
    "        self._execution_traces = None\n",
    "        if execution_traces is not None:\n",
    "            self._execution_traces = dict()\n",
    "            with open(corpus_root + execution_traces, \"r\") as et_file:\n",
    "                for line in et_file.readlines():\n",
    "                    tokens = line.split()\n",
    "                    if tokens[0] not in self._execution_traces:\n",
    "                        self._execution_traces[tokens[0]] = []\n",
    "                    if tokens[1] not in self._execution_traces:\n",
    "                        self._execution_traces[tokens[1]] = []\n",
    "\n",
    "                    self._execution_traces[tokens[0]].append(tokens[1])\n",
    "                    self._execution_traces[tokens[1]].append(tokens[0])\n",
    "\n",
    "        self._stop_words = []\n",
    "        self._stemmers = []\n",
    "\n",
    "        if languages is not None:\n",
    "            for language in languages:\n",
    "                # check if there's a stop word list\n",
    "                stop_words_path = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    \"test_data/config_corpus/\" + language + \"_stop_words.txt\",\n",
    "                )\n",
    "                try:\n",
    "                    with open(stop_words_path) as stop_words_file:\n",
    "                        self._stop_words += stop_words_file.read().split()\n",
    "                        print(\"Found stop words file: \" + language + \"_stop_words.txt\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No stop words found for language: [\" + language + \"]\")\n",
    "\n",
    "                # check if there's a stemmer\n",
    "                try:\n",
    "                    stemmer = SnowballStemmer(language)\n",
    "                    print(\n",
    "                        \"Detected natural language: [\"\n",
    "                        + language\n",
    "                        + \"], generating stemmer\"\n",
    "                    )\n",
    "                    self._stemmers.append(stemmer)\n",
    "                except ValueError:\n",
    "                    print(\n",
    "                        \"No natural language stemmer detected for language: [\"\n",
    "                        + language\n",
    "                        + \"]\"\n",
    "                    )\n",
    "\n",
    "        self._corpus_code = corpus_code\n",
    "\n",
    "    def _parse_artifacts(self, path, index):\n",
    "        \"\"\"\n",
    "        Reads and indexes artifacts from file or files given in path\n",
    "        Args:\n",
    "            path (str): Path to the file or folder from which to parse artifacts\n",
    "            index (list of str): Stores the identifiers for each artifact. This\n",
    "                list should be empty when this method is called (the method will\n",
    "                populate it).\n",
    "        Returns:\n",
    "            list of str: The list of artifacts\n",
    "        \"\"\"\n",
    "        root = self._corpus_root\n",
    "        print(\"Finding artifacts in: \" + root + path)\n",
    "        store = []\n",
    "        if os.path.isfile(root + path):\n",
    "            print(\"Getting artifacts by line from file\")\n",
    "            with open(\n",
    "                root + path, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n",
    "            ) as artifacts_file:\n",
    "                for line in artifacts_file.readlines():\n",
    "                    tokens = line.split()\n",
    "                    artifact = \" \".join(tokens[1:])\n",
    "                    index.append(tokens[0])\n",
    "                    store.append(artifact)\n",
    "                print(\"Read \" + str(len(store)) + \" artifacts from file\")\n",
    "        else:\n",
    "            print(\"Getting artifacts by file from directory\")\n",
    "            for element in os.listdir(root + path):\n",
    "                self._parse_artifacts_recur(path + \"/\", element, index, store)\n",
    "            print(\"Read \" + str(len(store)) + \" artifacts from directory\")\n",
    "\n",
    "        print()\n",
    "        return store\n",
    "\n",
    "    def _parse_artifacts_recur(self, subroot, element, index, store):\n",
    "        \"\"\"\n",
    "        Recursively searches for files in the directory given in the directory\n",
    "        path from the corpus root.\n",
    "        Args:\n",
    "            path (str): Path to a file or directory. If path is a file, this is\n",
    "                the base case and the file is read and an artifact is created\n",
    "                and indexed\n",
    "            index (list of str): Cumulatively stores the identifiers for each artifact.\n",
    "            store (list of str): Cumulatively stores the artifacts.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        root = self._corpus_root\n",
    "        full_path = root + subroot + element\n",
    "\n",
    "        if os.path.isfile(full_path):\n",
    "            extension = element[element.index(\".\") + 1 :].lower()\n",
    "\n",
    "            if self._filetype_whitelist and extension not in self._filetype_whitelist:\n",
    "                return\n",
    "            if self._filetype_blacklist and extension in self._filetype_blacklist:\n",
    "                return\n",
    "\n",
    "            element_clean = element.replace(\"/\", \".\")\n",
    "            index.append(element_clean)\n",
    "            with open(\n",
    "                full_path, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n",
    "            ) as artifact_file:\n",
    "                store.append(artifact_file.read())\n",
    "\n",
    "        else:\n",
    "            for sub_element in os.listdir(root + subroot + element):\n",
    "                sub_element = element + \"/\" + sub_element\n",
    "                self._parse_artifacts_recur(subroot, sub_element, index, store)\n",
    "\n",
    "    # Getters\n",
    "    def get_sources(self):\n",
    "        return self._sources\n",
    "\n",
    "    def get_targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    def get_source_names(self):\n",
    "        return self._source_index\n",
    "\n",
    "    def get_target_names(self):\n",
    "        return self._target_index\n",
    "\n",
    "    def get_source_artifact_at_index(self, index):\n",
    "        return self._sources[index]\n",
    "\n",
    "    def get_target_artifact_at_index(self, index):\n",
    "        return self._targets[index]\n",
    "\n",
    "    def get_source_artifact_by_name(self, name):\n",
    "        try:\n",
    "            index = self._source_index.index(name)\n",
    "            return self._sources[index]\n",
    "        except ValueError:\n",
    "            print(\"Source artifact '\" + name + \"' not found\")\n",
    "            return None\n",
    "\n",
    "    def get_target_artifact_by_name(self, name):\n",
    "        try:\n",
    "            index = self._target_index.index(name)\n",
    "            return self._targets[index]\n",
    "        except ValueError:\n",
    "            print(\"Target artifact '\" + name + \"' not found\")\n",
    "            return None\n",
    "\n",
    "    def get_source_name_by_index(self, index):\n",
    "        return self._source_index[index]\n",
    "\n",
    "    def get_target_name_by_index(self, index):\n",
    "        return self._target_index[index]\n",
    "\n",
    "    def get_truth_value(self, source, target):\n",
    "        if self._truth is not None:\n",
    "            return self._truth[source][target]\n",
    "\n",
    "    def get_truth_dict(self):\n",
    "        return dict(self._truth)\n",
    "\n",
    "    def get_execution_trace(self, artifact):\n",
    "        if artifact in self._execution_traces:\n",
    "            return self._execution_traces[artifact]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_all_execution_traces(self):\n",
    "        return dict(self._execution_traces)\n",
    "\n",
    "    def get_stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    def get_stemmers(self):\n",
    "        return self._stemmers\n",
    "\n",
    "    def get_subset(self, percent):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        total_num_links = len(self._all_links)\n",
    "        num_links = int(total_num_links * (percent / 100))\n",
    "\n",
    "        link_subset = random.sample(self._all_links, num_links)\n",
    "\n",
    "        link_subset_dict = dict()\n",
    "\n",
    "        for link in link_subset:\n",
    "            source = link[0]\n",
    "            target = link[1]\n",
    "            link_status = self.get_truth_value(source, target)\n",
    "\n",
    "            if source not in link_subset_dict:\n",
    "                link_subset_dict[source] = dict()\n",
    "\n",
    "            link_subset_dict[source][target] = link_status\n",
    "\n",
    "        return link_subset_dict\n",
    "\n",
    "    def get_subsets(self, percent, n_trials):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        subsets = []\n",
    "        for n in range(n_trials):\n",
    "            subsets.append(self.get_subset(percent))\n",
    "        return subsets\n",
    "\n",
    "    def get_positive_link_subset(self, num_sources, accuracy=0.75):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        source_samples = random.sample(self.get_source_names(), num_sources)\n",
    "        link_subset_dict = dict()\n",
    "\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        for source in source_samples:\n",
    "            link_subset_dict[source] = dict()\n",
    "\n",
    "            recall_rate = random.gauss(accuracy, 0.1)\n",
    "            if recall_rate < 0:\n",
    "                recall_rate = 0\n",
    "            elif recall_rate > 1:\n",
    "                recall_rate = 1\n",
    "\n",
    "            actual_links = [\n",
    "                target\n",
    "                for target in targets\n",
    "                if self.get_truth_value(source, target) == 1\n",
    "            ]\n",
    "\n",
    "            num_links_found = int(len(actual_links) * recall_rate)\n",
    "\n",
    "            for target in targets:\n",
    "                link_subset_dict[source][target] = 0\n",
    "\n",
    "            links_found = random.sample(actual_links, num_links_found)\n",
    "            for target in links_found:\n",
    "                link_subset_dict[source][target] = 1\n",
    "\n",
    "        return link_subset_dict\n",
    "\n",
    "    def get_positive_link_subsets(self, num_sources, n_trials, accuracy=0.75):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        subsets = []\n",
    "        for n in range(n_trials):\n",
    "            subsets.append(\n",
    "                self.get_positive_link_subset(num_sources, accuracy=accuracy)\n",
    "            )\n",
    "\n",
    "        return subsets\n",
    "\n",
    "    def get_link_sample(self, num_positive=5, num_negative=5):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        sources = self.get_source_names()\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        positives = []\n",
    "        negatives = []\n",
    "\n",
    "        for source in sources:\n",
    "            for target in targets:\n",
    "                if self.get_truth_value(source, target) == 1:\n",
    "                    positives.append((source, target))\n",
    "                else:\n",
    "                    negatives.append((source, target))\n",
    "\n",
    "        if len(positives) < num_positive:\n",
    "            print(\n",
    "                \"Warning: \"\n",
    "                + str(num_positive)\n",
    "                + \" positive links were requested, but only \"\n",
    "                + str(len(positives))\n",
    "                + \" positive links exist.\"\n",
    "            )\n",
    "            positive_subset = positives\n",
    "        else:\n",
    "            positive_subset = random.sample(positives, num_positive)\n",
    "\n",
    "        if len(negatives) < num_negative:\n",
    "            print(\n",
    "                \"Warning: \"\n",
    "                + str(num_negative)\n",
    "                + \" negative links were requested, but only \"\n",
    "                + str(len(negatives))\n",
    "                + \" negative links exist.\"\n",
    "            )\n",
    "            negative_subset = negatives\n",
    "        else:\n",
    "            negative_subset = random.sample(negatives, num_negative)\n",
    "\n",
    "        return positive_subset + negative_subset\n",
    "\n",
    "    def get_corpus_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def get_corpus_code(self):\n",
    "        if self._corpus_code is not None:\n",
    "            return self._corpus_code\n",
    "        else:\n",
    "            print(\"No corpus code found for: \" + self.get_corpus_name())\n",
    "\n",
    "    def get_corpus_root(self):\n",
    "        return self._corpus_root\n",
    "\n",
    "    def get_raw_string(self):\n",
    "        output = \"\"\n",
    "        for doc in self._sources + self._targets:\n",
    "            output += doc + \"\\n\"\n",
    "        return output\n",
    "\n",
    "    def generate_raw_file(self, output_filename=None):\n",
    "        if output_filename is None:\n",
    "            output_filename = self._corpus_root + self._corpus_code + \"_raw_corpus.txt\"\n",
    "        with open(output_filename, \"w+\") as output_file:\n",
    "            output_file.write(self.get_raw_string())\n",
    "\n",
    "    def verify_datastore(self, datastore_manager):\n",
    "        sources = self.get_source_names()\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        complete = True\n",
    "        for source in sources:\n",
    "            for target in targets:\n",
    "                if not datastore_manager.file_exists(source, target, \"NUTS\"):\n",
    "                    print(\"Missing file for (\" + source + \", \" + target + \")\")\n",
    "                    complete = False\n",
    "\n",
    "        if complete:\n",
    "            print(\"All links have been generated for \" + self.get_corpus_name() + \"!\")\n",
    "        else:\n",
    "            print(\"Some links are missing for \" + self.get_corpus_name() + \"...\")\n",
    "\n",
    "    # STATIC\n",
    "    @classmethod\n",
    "    def get_preset_corpus(cls, corpus_code):\n",
    "        if corpus_code == \"LibEST\":\n",
    "            corpus_code = \"0_1\"\n",
    "        elif corpus_code == \"EBT\":\n",
    "            corpus_code = \"1_1\"\n",
    "        elif corpus_code == \"eTOUR\":\n",
    "            corpus_code = \"2_0\"\n",
    "        elif corpus_code == \"iTrust\":\n",
    "            corpus_code = \"3_0\"\n",
    "        elif corpus_code == \"Albergate\":\n",
    "            corpus_code = \"4_0\"\n",
    "        elif corpus_code == \"SMOS\":\n",
    "            corpus_code = \"5_0\"\n",
    "\n",
    "        try:\n",
    "            separator_index = corpus_code.index(\"_\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid corpus code: \" + corpus_code)\n",
    "            return\n",
    "\n",
    "        dataset = corpus_code[:separator_index]\n",
    "        subset = corpus_code[separator_index + 1 :]\n",
    "\n",
    "        dataset_name = None\n",
    "        modifier = None\n",
    "        source_path = None\n",
    "        target_path = None\n",
    "        truth_path = None\n",
    "        execution_traces = None\n",
    "        languages = None\n",
    "\n",
    "        if dataset == \"0\":\n",
    "            dataset_name = \"LibEST\"\n",
    "            source_path = \"requirements\"\n",
    "            languages = [\"english\", \"C\"]\n",
    "            execution_traces = \"execution_traces.txt\"\n",
    "\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(RQ to Code and Tests)\"\n",
    "                target_path = [\"source_code\", \"test\"]\n",
    "                truth_path = [\"req_to_code_ground.txt\", \"req_to_test_ground.txt\"]\n",
    "\n",
    "            elif subset == \"1\":\n",
    "                modifier = \"(RQ to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"req_to_code_ground.txt\"\n",
    "\n",
    "            elif subset == \"2\":\n",
    "                modifier = \"(RQ to Tests)\"\n",
    "                target_path = \"test\"\n",
    "                truth_path = \"req_to_test_ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == \"1\":\n",
    "            dataset_name = \"EBT\"\n",
    "            source_path = \"requirements.txt\"\n",
    "            languages = [\"english\"]\n",
    "\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(RQ to Code and Tests)\"\n",
    "                target_path = [\"source_code\", \"test_cases.txt\"]\n",
    "                truth_path = \"both_ground.txt\"\n",
    "\n",
    "            elif subset == \"1\":\n",
    "                modifier = \"(RQ to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"code_ground.txt\"\n",
    "\n",
    "            elif subset == \"2\":\n",
    "                modifier = \"(RQ to Test)\"\n",
    "                target_path = \"test_cases.txt\"\n",
    "                truth_path = \"tests_ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == \"2\":\n",
    "            dataset_name = \"eTOUR\"\n",
    "            source_path = \"use_cases_with_translation\"\n",
    "            languages = [\"english\", \"italian\", \"java\"]\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(UC to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == \"3\":\n",
    "            dataset_name = \"iTrust\"\n",
    "            source_path = \"use_cases\"\n",
    "            languages = [\"english\", \"java\"]\n",
    "\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(UC to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == \"4\":\n",
    "            dataset_name = \"Albergate\"\n",
    "            source_path = \"requirements\"\n",
    "            languages = [\"italian\", \"java\"]\n",
    "\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(RQ to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == \"5\":\n",
    "            dataset_name = \"SMOS\"\n",
    "            source_path = \"use_cases\"\n",
    "            languages = [\"italian\", \"java\"]\n",
    "\n",
    "            if subset == \"0\":\n",
    "                modifier = \"(UC to Code)\"\n",
    "                target_path = \"source_code\"\n",
    "                truth_path = \"ground.txt\"\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "        else:\n",
    "            print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "            return\n",
    "\n",
    "        file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        corpus_root = file_path + \"/../../data/raw/\" + dataset_name + \"_semeru_format/\"\n",
    "\n",
    "        corpus_name = dataset_name + \" (\" + corpus_code + \")\"\n",
    "        filetype_whitelist = [\"java\", \"txt\", \"jsp\", \"h\", \"c\"]\n",
    "\n",
    "        corpus = Corpus(\n",
    "            corpus_name,\n",
    "            corpus_root,\n",
    "            source_path,\n",
    "            target_path,\n",
    "            truth_path,\n",
    "            execution_traces=execution_traces,\n",
    "            corpus_code=corpus_code,\n",
    "            languages=languages,\n",
    "            filetype_whitelist=filetype_whitelist,\n",
    "        )\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_preset_corpus_codes(cls):\n",
    "        codes = [\"0_0\", \"0_1\", \"0_2\", \"1_0\", \"1_1\", \"1_2\", \"2_0\", \"3_0\", \"4_0\", \"5_0\"]\n",
    "        return codes\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_preset_corpora(cls):\n",
    "        codes = Corpus.get_all_preset_corpus_codes()\n",
    "\n",
    "        return [Corpus.get_preset_corpus(code) for code in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is : /tf/main/nbs\n",
      "Directory name is : nbs\n"
     ]
    }
   ],
   "source": [
    "# Exploring!\n",
    "dirpath = os.getcwd()\n",
    "print(\"Current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(dirpath, \"test_data\")\n",
    "data_folder = os.path.join(data_folder, \"LibEST_semeru_format/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding artifacts in: /tf/main/nbs/test_data/LibEST_semeru_format/requirements\n",
      "Getting artifacts by file from directory\n",
      "Read 52 artifacts from directory\n",
      "\n",
      "Finding artifacts in: /tf/main/nbs/test_data/LibEST_semeru_format/source_code\n",
      "Getting artifacts by file from directory\n",
      "Read 14 artifacts from directory\n",
      "\n",
      "Found stop words file: english_stop_words.txt\n",
      "Detected natural language: [english], generating stemmer\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(\n",
    "    name=\"libest\",\n",
    "    corpus_root=data_folder,\n",
    "    source_path=\"requirements\",\n",
    "    target_path=\"source_code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.get_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\n",
    "    os.path.join(os.getcwd(), \"test_data/config_corpus/\" + language + \"_stop_words.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corpus_creation():\n",
    "    corpus = Corpus(\n",
    "        name,\n",
    "        corpus_root=self._data_folder,\n",
    "        source_path=\"requirements\",\n",
    "        target_path=\"source_code\",\n",
    "    )\n",
    "    assert len(corpus.get_sources()) == 52\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCorpus(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        dirpath = os.getcwd()\n",
    "        self._data_folder = os.path.join(dirpath, \"test_data\")\n",
    "        self._data_folder = os.path.join(data_folder, \"LibEST_semeru_format/\")\n",
    "        pass\n",
    "\n",
    "    def test_corpus_creation(self):\n",
    "        corpus = Corpus(\n",
    "            name,\n",
    "            corpus_root=self._data_folder,\n",
    "            source_path=\"requirements\",\n",
    "            target_path=\"source_code\",\n",
    "        )\n",
    "        self.assertEqual(corpus.get_sources(), 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus.get_sources()) == 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 0 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /root/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/root/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ERROR: /root/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/root/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"hello\")\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

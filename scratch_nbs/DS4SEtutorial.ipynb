{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DS4SEtutorial.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu8SN9RmzoeQ"
   },
   "source": [
    "# DS4SE Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HkdzOKD0HyT"
   },
   "source": [
    "This quick tutorial uses [DS4SE API](https://pypi.org/project/ds4se/) to:\n",
    "1. Calculate traceability value between one pair of artifacts.\n",
    "2. For source and target artifact class in Libest dataset, calculate:\n",
    "  \n",
    "  >1) the number of documents in each class\n",
    "\n",
    "  >2) the vocab size of each class\n",
    "\n",
    "  >3) the average number of token in each documents of each class\n",
    "\n",
    "  >4) the top three most frequent tokens in source and target artifact classes\n",
    "\n",
    "  >5) the top three most frequent tokens in source artifact class\n",
    "\n",
    "  >6) the number of shared vocabulary between source and target artifact class\n",
    "\n",
    "  >7) the cross entropy value of source and target artifact class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nztAxDWrA_PW"
   },
   "source": [
    "This is a quick introduction on how to use the DS4SE API, to follow this tutorial in Google Colab, click the right arrow button in each cell in sequence or click Runtime-> Run all to run all the cells at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX6mQDUm0ZMo"
   },
   "source": [
    "Download and install dependent libraries of DS4SE."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tuX5a8RE0gUl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5c36169-9fbf-4f63-e65d-5016896f7e2a"
   },
   "source": [
    "!pip install --upgrade gensim\n",
    "!pip install nbdev\n",
    "!pip install sentencepiece\n",
    "!pip install dit"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: nbdev in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from nbdev) (5.3.5)\n",
      "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from nbdev) (5.0.8)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from nbdev) (4.10.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nbdev) (3.13)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from nbdev) (19.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from nbdev) (20.4)\n",
      "Requirement already satisfied: nbconvert<6 in /usr/local/lib/python3.6/dist-packages (from nbdev) (5.6.1)\n",
      "Requirement already satisfied: fastcore>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from nbdev) (1.3.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev) (2.8.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev) (5.1.1)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev) (4.6.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev) (19.0.2)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev) (2.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->nbdev) (5.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev) (1.15.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (0.4.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (3.2.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (1.4.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (2.11.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev) (2.6.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->jupyter-client->nbdev) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (50.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (1.0.18)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (4.8.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert<6->nbdev) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert<6->nbdev) (1.1.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->nbdev) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->nbdev) (0.6.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
      "Requirement already satisfied: dit in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
      "Requirement already satisfied: boltons in /usr/local/lib/python3.6/dist-packages (from dit) (20.2.1)\n",
      "Requirement already satisfied: debtcollector in /usr/local/lib/python3.6/dist-packages (from dit) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from dit) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.4.1)\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (from dit) (1.0.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from dit) (1.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from dit) (2.5)\n",
      "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dit) (0.5.5)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (5.5.1)\n",
      "Requirement already satisfied: wrapt>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from debtcollector->dit) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from prettytable->dit) (50.3.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prettytable->dit) (0.2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->dit) (4.4.2)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX8MNChm0V1Y"
   },
   "source": [
    "Download and install DS4SE. Import TensorFlow into your program:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCbsOumw0KsV",
    "outputId": "49221fe2-e13b-4106-cc39-d9f07b564869"
   },
   "source": [
    "pip install ds4se"
   ],
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ds4se in /usr/local/lib/python3.6/dist-packages (0.2.1)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gvXnbVgj04J4"
   },
   "source": [
    "import ds4se.facade as facade"
   ],
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFE_rPVe2wyM"
   },
   "source": [
    "Import other libraries needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tZCODLyX23V4"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQke9-cc03Me"
   },
   "source": [
    "Load and prapare [Libest dataset](https://github.com/WM-SEMERU/ds4se/tree/master/nbs/test_data). Convert the column name in which actual file content is stored into \"contents\".\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WbwfiCXO1-hm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af83fd0e-ddd6-4e36-d651-ae21cc42297f"
   },
   "source": [
    "!wget https://raw.githubusercontent.com/WM-SEMERU/ds4se/SE_Proj2_Facade/nbs/test_data/%5Blibest-pre-req%5D.csv\n",
    "!wget https://raw.githubusercontent.com/WM-SEMERU/ds4se/SE_Proj2_Facade/nbs/test_data/%5Blibest-pre-tc%5D.csv"
   ],
   "execution_count": 59,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2020-11-21 04:17:44--  https://raw.githubusercontent.com/WM-SEMERU/ds4se/SE_Proj2_Facade/nbs/test_data/%5Blibest-pre-req%5D.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60790 (59K) [text/plain]\n",
      "Saving to: ‘[libest-pre-req].csv.1’\n",
      "\n",
      "\r[libest-pre-req].cs   0%[                    ]       0  --.-KB/s               \r[libest-pre-req].cs 100%[===================>]  59.37K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-11-21 04:17:44 (4.28 MB/s) - ‘[libest-pre-req].csv.1’ saved [60790/60790]\n",
      "\n",
      "--2020-11-21 04:17:44--  https://raw.githubusercontent.com/WM-SEMERU/ds4se/SE_Proj2_Facade/nbs/test_data/%5Blibest-pre-tc%5D.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 343900 (336K) [text/plain]\n",
      "Saving to: ‘[libest-pre-tc].csv.1’\n",
      "\n",
      "[libest-pre-tc].csv 100%[===================>] 335.84K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-11-21 04:17:45 (8.32 MB/s) - ‘[libest-pre-tc].csv.1’ saved [343900/343900]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OhNDeEMe2dJw"
   },
   "source": [
    "source_file = pd.read_csv(\"[libest-pre-req].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "target_file = pd.read_csv(\"[libest-pre-tc].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "source_file = source_file.rename(columns={\"text\":\"contents\"})\n",
    "target_file = target_file.rename(columns={\"text\":\"contents\"})"
   ],
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-LarMKf302u"
   },
   "source": [
    "Create a pandas dataframe to store the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-RJ4hNGA4R1q"
   },
   "source": [
    "d = {'source': [], 'target': [], 'distance':[],'similarity/traceability':[]}\n",
    "output_df = pd.DataFrame(data=d)"
   ],
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-Oo4UlG4mxW"
   },
   "source": [
    "Retrive one element from source artifact and one element from target artifact to calculate traceability. Store id information for reference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mHLKchLc5k2t"
   },
   "source": [
    "source_id = source_file[\"ids\"][1].split('/')[-1]\n",
    "target_id = target_file[\"ids\"][1].split('/')[-1]\n",
    "source_string = source_file[\"contents\"][1]\n",
    "target_string = target_file[\"contents\"][1]"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTyIwRxs5rU5"
   },
   "source": [
    "Call TraceLinkValue method to calcuate the distance and traceability values of this pair. In this example we used word2vec technique."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgVMeJNC4m_U",
    "outputId": "b9b50888-25c8-4c80-8e83-60b31271d0a2"
   },
   "source": [
    "TLV = facade.TraceLinkValue(source_string, target_string, \"word2vec\")\n",
    "distance = TLV[0]\n",
    "traceability = TLV[1]"
   ],
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2020-11-21 04:17:45,247 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-21 04:17:45,256 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-21 04:17:45,257 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-21 04:17:45,327 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-21 04:17:45,330 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-21 04:17:45,332 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-21 04:17:45,332 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-21 04:17:45,333 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-21 04:17:45,334 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-21 04:17:45,366 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-21 04:17:45,370 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f3cc1334518>\n",
      "2020-11-21 04:17:45,374 : INFO : iterating over columns in dictionary order\n",
      "2020-11-21 04:17:45,379 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-21 04:17:45,624 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-21 04:17:45,746 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-21 04:17:45,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-21 04:17:45,768 : INFO : built Dictionary(502 unique tokens: ['accompani', 'addit', 'agre', 'agreement', 'algorithm']...) from 2 documents (total 2134 corpus positions)\n",
      "2020-11-21 04:17:49,106 : INFO : Computed distances or similarities ('source', 'target')[[0.4005029238604272, 0.7140292126228072]]\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9cgdPwy6Vm8"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNMssRvy5iw5",
    "outputId": "93562c5c-0474-4f03-d68a-6ea086f9a895"
   },
   "source": [
    "print(\"The traceability value between artifacts {} and {} is {}\".format(source_id,target_id,format(traceability,'.2f')))"
   ],
   "execution_count": 64,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The traceability value between artifacts RQ46-pre.txt and us3496.c is 0.71\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m998Sfi962DP"
   },
   "source": [
    "Call **NumDoc** method to count the number of documents in each artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ed7i00gM69Du"
   },
   "source": [
    "num_docs = facade.NumDoc(source_file, target_file)"
   ],
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLHGhG4k7Bts"
   },
   "source": [
    "Display the number of documents result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtTt1X8e7Fdm",
    "outputId": "f0c77424-d4be-4d9a-96f6-71c0604426c7"
   },
   "source": [
    "print(\"Source artifacts contains {} documents, Target artifacts contains {} documents.\".format(num_docs[0],num_docs[1]))"
   ],
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Source artifacts contains 52 documents, Target artifacts contains 21 documents.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4PxltUd8Sgz"
   },
   "source": [
    "Call **VocabSize** method to count the vocabulary size of each artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H4qGzeoX8Sgz"
   },
   "source": [
    "vocab_size = facade.VocabSize(source_file,target_file)"
   ],
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nex61OxM8Sgz"
   },
   "source": [
    "Display the vocabulary size of each artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxocesHg8Sgz",
    "outputId": "09d435c5-4fcb-4169-f65d-debb3322733c"
   },
   "source": [
    "print(\"Source artifacts's vocab size is {}. Target artifacts's vocab size is {}.\".format(vocab_size[0],vocab_size[1]))"
   ],
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Source artifacts's vocab size is 2349. Target artifacts's vocab size is 3168.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42wuvP6_IF2v"
   },
   "source": [
    "Computes the average number of token in each class and also the difference between them using **AverageToken** method:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "edJEEtldAWtT"
   },
   "source": [
    "token = facade.AverageToken(source_file, target_file)"
   ],
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dnF8sAu-IBL"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR-gK8Y5-Ied",
    "outputId": "8e88771d-1aec-45f4-a9bf-3ed580ac9f68"
   },
   "source": [
    "print(\"On average, each document in source artifact class contains {} tokens and each document in target artifact class contains {} tokens \".format(token[0],token[1]))"
   ],
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "On average, each document in source artifact class contains 365.21153846153845 tokens and each document in target artifact class contains 4970.476190476191 tokens \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghxkE0X2G45Y"
   },
   "source": [
    "To find out the most frequent token in both source and target artifacts, use **VocabShared** method"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJBiqZDVGxqi"
   },
   "source": [
    "vocab_shared = facade.VocabShared(source_file, target_file)"
   ],
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1hhXMe4-JWi"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyelaXi6-Jfo",
    "outputId": "d3f44dd6-7fa7-4cd1-ed23-71b808d98f34"
   },
   "source": [
    "print(\"the top three most frequent token used in two artifact classes and their corresponding count and frenquency is:\")\n",
    "vocab_shared"
   ],
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "the top three most frequent token used in two artifact classes and their corresponding count and frenquency is:\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'1': [2903, 0.02353065144969239],\n",
       " '8': [2241, 0.01816472266578045],\n",
       " '▁': [53876, 0.43669906217830773]}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 72
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArklHjhx-lPs"
   },
   "source": [
    "Use **Vocab** method for the most frequent token in just source artifacts class:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cGopkMgwHDaj"
   },
   "source": [
    "vocab = facade.Vocab(source_file)"
   ],
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWQ5LLlC-KV5"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIn2AtXZ-Kea",
    "outputId": "a76b160e-b286-48c1-dc3a-d438e3cdcab9"
   },
   "source": [
    "print(\"the top three most frequent token used in source artifact classes and their corresponding count and frenquency is:\")\n",
    "vocab"
   ],
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "the top three most frequent token used in source artifact classes and their corresponding count and frenquency is:\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'client': [291, 0.01532304775946501],\n",
       " 'est': [281, 0.014796482544363119],\n",
       " '▁': [8912, 0.4692749196988047]}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 74
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X-PHjVCmHS5J"
   },
   "source": [
    "sharedvocabsize = facade.SharedVocabSize(source_file, target_file)"
   ],
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K85Nj-W-K_s"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5dG4yQC-LKk",
    "outputId": "dd3a702c-4cca-41dc-8c05-f8816436b056"
   },
   "source": [
    "print(\"the number of shared token between source and target artifact classes is {}\".format(sharedvocabsize))"
   ],
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "the number of shared token between source and target artifact classes is 5042\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4paqRmalHac8"
   },
   "source": [
    "Use **CrossEntropy** methods to calcualte cross entropy of source and target class:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1NthD6LdHs8D"
   },
   "source": [
    "entropy = facade.CrossEntropy(source_file, target_file)"
   ],
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks8tkaIa9ov2"
   },
   "source": [
    "Display the result:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcWh09eH9sLB",
    "outputId": "5617bb7c-ad40-4f8a-ffd7-e4c4583ab7d3"
   },
   "source": [
    "print(\"The cross entropy value of source and target artifacts is {}\".format(format(entropy,\".2f\")))"
   ],
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The cross entropy value of source and target artifacts is 6.16\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}

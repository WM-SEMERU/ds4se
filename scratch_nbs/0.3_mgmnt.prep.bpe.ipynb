{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Preprocessing\n",
    "\n",
    "> This module comprises all preprocessing techniques applied to software artifacts:\n",
    ">\n",
    ">> Text-based Artifacts: Classical preprocessing (stemming, lemas, etc) and BPE Binary Artifacts:\n",
    ">\n",
    ">> To Do Vision-based Artifacts:\n",
    ">\n",
    ">> To Do Parsing: Techniques to control and manipulate source code (complete with deep generator project)\n",
    ">\n",
    "> Author: @danaderp Jun 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.8.1\n"
     ]
    }
   ],
   "source": [
    "#! pip install dit\n",
    "#! pip install fastprogress\n",
    "#! pip install nltk\n",
    "! pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-01 14:30:59--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt’\n",
      "\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2020-07-01 14:31:00 (5.91 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just for testing\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sentencepiece as spm\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import List, Set, Callable, Tuple, Dict, Optional\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from string import punctuation\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"system\": \"codesearchnet\",\n",
    "    \"saving_path\": \"test_data/sentencepiece/\",\n",
    "    \"language\": \"english\",\n",
    "    \"wiki_size\": 60000,\n",
    "    \"bpe_filename\": \"test_data/sentencepiece/py_java_bpe_training.txt\",\n",
    "    \"model_prefix\": \"test_data/sentencepiece/wiki_py_java_bpe_128k\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_bpe_training(np_str, mode=\"w\"):\n",
    "    with open(params[\"bpe_filename\"], mode) as f:\n",
    "        # Writing data to a file\n",
    "        f.write(\"\\n\".join(np_str))  # adding space between elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Sentence Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    \"--input=test_data/botchan.txt --model_prefix=m --vocab_size=2000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"m.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[209, 31, 9, 375, 586]\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(\"This is a test\"))\n",
    "print(sp.encode_as_ids(\"This is a test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces([\"▁This\", \"▁is\", \"▁a\", \"▁t\", \"est\"]))\n",
    "print(sp.decode_ids([209, 31, 9, 375, 586]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "▁This\n",
      "209\n",
      "0\n",
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(209))\n",
    "print(sp.piece_to_id(\"▁This\"))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id(\"__MUST_BE_UNKNOWN__\"))\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "    print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SettingUp Software Corpora from CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files = sorted(Path(\"codesearch/python/\").glob(\"**/*.gz\"))\n",
    "java_files = sorted(Path(\"codesearch/java/\").glob(\"**/*.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_long_list = [\n",
    "    \"repo\",\n",
    "    \"path\",\n",
    "    \"url\",\n",
    "    \"code\",\n",
    "    \"code_tokens\",\n",
    "    \"docstring\",\n",
    "    \"docstring_tokens\",\n",
    "    \"language\",\n",
    "    \"partition\",\n",
    "]\n",
    "\n",
    "columns_short_list = [\"code_tokens\", \"docstring_tokens\", \"language\", \"partition\"]\n",
    "\n",
    "\n",
    "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_searchnet_df = jsonl_list_to_dataframe(python_files)\n",
    "java_searchnet_df = jsonl_list_to_dataframe(java_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>url</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>language</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReactiveX/RxJava</td>\n",
       "      <td>src/main/java/io/reactivex/internal/observers/...</td>\n",
       "      <td>https://github.com/ReactiveX/RxJava/blob/ac841...</td>\n",
       "      <td>protected final void fastPathOrderedEmit(U val...</td>\n",
       "      <td>[protected, final, void, fastPathOrderedEmit, ...</td>\n",
       "      <td>Makes sure the fast-path emits in order.\\n@par...</td>\n",
       "      <td>[Makes, sure, the, fast, -, path, emits, in, o...</td>\n",
       "      <td>java</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ReactiveX/RxJava</td>\n",
       "      <td>src/main/java/io/reactivex/Observable.java</td>\n",
       "      <td>https://github.com/ReactiveX/RxJava/blob/ac841...</td>\n",
       "      <td>@CheckReturnValue\\n    @NonNull\\n    @Schedule...</td>\n",
       "      <td>[@, CheckReturnValue, @, NonNull, @, Scheduler...</td>\n",
       "      <td>Mirrors the one ObservableSource in an Iterabl...</td>\n",
       "      <td>[Mirrors, the, one, ObservableSource, in, an, ...</td>\n",
       "      <td>java</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ReactiveX/RxJava</td>\n",
       "      <td>src/main/java/io/reactivex/Observable.java</td>\n",
       "      <td>https://github.com/ReactiveX/RxJava/blob/ac841...</td>\n",
       "      <td>@SuppressWarnings(\"unchecked\")\\n    @CheckRetu...</td>\n",
       "      <td>[@, SuppressWarnings, (, \"unchecked\", ), @, Ch...</td>\n",
       "      <td>Mirrors the one ObservableSource in an array o...</td>\n",
       "      <td>[Mirrors, the, one, ObservableSource, in, an, ...</td>\n",
       "      <td>java</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ReactiveX/RxJava</td>\n",
       "      <td>src/main/java/io/reactivex/Observable.java</td>\n",
       "      <td>https://github.com/ReactiveX/RxJava/blob/ac841...</td>\n",
       "      <td>@SuppressWarnings({ \"unchecked\", \"rawtypes\" })...</td>\n",
       "      <td>[@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...</td>\n",
       "      <td>Concatenates elements of each ObservableSource...</td>\n",
       "      <td>[Concatenates, elements, of, each, ObservableS...</td>\n",
       "      <td>java</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ReactiveX/RxJava</td>\n",
       "      <td>src/main/java/io/reactivex/Observable.java</td>\n",
       "      <td>https://github.com/ReactiveX/RxJava/blob/ac841...</td>\n",
       "      <td>@SuppressWarnings({ \"unchecked\", \"rawtypes\" })...</td>\n",
       "      <td>[@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...</td>\n",
       "      <td>Returns an Observable that emits the items emi...</td>\n",
       "      <td>[Returns, an, Observable, that, emits, the, it...</td>\n",
       "      <td>java</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               repo                                               path  \\\n",
       "0  ReactiveX/RxJava  src/main/java/io/reactivex/internal/observers/...   \n",
       "1  ReactiveX/RxJava         src/main/java/io/reactivex/Observable.java   \n",
       "2  ReactiveX/RxJava         src/main/java/io/reactivex/Observable.java   \n",
       "3  ReactiveX/RxJava         src/main/java/io/reactivex/Observable.java   \n",
       "4  ReactiveX/RxJava         src/main/java/io/reactivex/Observable.java   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://github.com/ReactiveX/RxJava/blob/ac841...   \n",
       "1  https://github.com/ReactiveX/RxJava/blob/ac841...   \n",
       "2  https://github.com/ReactiveX/RxJava/blob/ac841...   \n",
       "3  https://github.com/ReactiveX/RxJava/blob/ac841...   \n",
       "4  https://github.com/ReactiveX/RxJava/blob/ac841...   \n",
       "\n",
       "                                                code  \\\n",
       "0  protected final void fastPathOrderedEmit(U val...   \n",
       "1  @CheckReturnValue\\n    @NonNull\\n    @Schedule...   \n",
       "2  @SuppressWarnings(\"unchecked\")\\n    @CheckRetu...   \n",
       "3  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...   \n",
       "4  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, fastPathOrderedEmit, ...   \n",
       "1  [@, CheckReturnValue, @, NonNull, @, Scheduler...   \n",
       "2  [@, SuppressWarnings, (, \"unchecked\", ), @, Ch...   \n",
       "3  [@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...   \n",
       "4  [@, SuppressWarnings, (, {, \"unchecked\", ,, \"r...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0  Makes sure the fast-path emits in order.\\n@par...   \n",
       "1  Mirrors the one ObservableSource in an Iterabl...   \n",
       "2  Mirrors the one ObservableSource in an array o...   \n",
       "3  Concatenates elements of each ObservableSource...   \n",
       "4  Returns an Observable that emits the items emi...   \n",
       "\n",
       "                                    docstring_tokens language partition  \n",
       "0  [Makes, sure, the, fast, -, path, emits, in, o...     java      test  \n",
       "1  [Mirrors, the, one, ObservableSource, in, an, ...     java      test  \n",
       "2  [Mirrors, the, one, ObservableSource, in, an, ...     java      test  \n",
       "3  [Concatenates, elements, of, each, ObservableS...     java      test  \n",
       "4  [Returns, an, Observable, that, emits, the, it...     java      test  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_searchnet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496688, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_searchnet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain = python_searchnet_df[python_searchnet_df.partition.eq(\"train\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412178, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javatrain = java_searchnet_df[java_searchnet_df.partition.eq(\"train\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454451, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "javatrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>url</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>language</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ageitgey/face_recognition</td>\n",
       "      <td>examples/face_recognition_knn.py</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "      <td>def train(train_dir, model_save_path=None, n_n...</td>\n",
       "      <td>[def, train, (, train_dir, ,, model_save_path,...</td>\n",
       "      <td>Trains a k-nearest neighbors classifier for fa...</td>\n",
       "      <td>[Trains, a, k, -, nearest, neighbors, classifi...</td>\n",
       "      <td>python</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ageitgey/face_recognition</td>\n",
       "      <td>examples/face_recognition_knn.py</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "      <td>def predict(X_img_path, knn_clf=None, model_pa...</td>\n",
       "      <td>[def, predict, (, X_img_path, ,, knn_clf, =, N...</td>\n",
       "      <td>Recognizes faces in given image using a traine...</td>\n",
       "      <td>[Recognizes, faces, in, given, image, using, a...</td>\n",
       "      <td>python</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ageitgey/face_recognition</td>\n",
       "      <td>examples/face_recognition_knn.py</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "      <td>def show_prediction_labels_on_image(img_path, ...</td>\n",
       "      <td>[def, show_prediction_labels_on_image, (, img_...</td>\n",
       "      <td>Shows the face recognition results visually.\\n...</td>\n",
       "      <td>[Shows, the, face, recognition, results, visua...</td>\n",
       "      <td>python</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ageitgey/face_recognition</td>\n",
       "      <td>face_recognition/api.py</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...</td>\n",
       "      <td>[def, _rect_to_css, (, rect, ), :, return, rec...</td>\n",
       "      <td>Convert a dlib 'rect' object to a plain tuple ...</td>\n",
       "      <td>[Convert, a, dlib, rect, object, to, a, plain,...</td>\n",
       "      <td>python</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ageitgey/face_recognition</td>\n",
       "      <td>face_recognition/api.py</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "      <td>def _trim_css_to_bounds(css, image_shape):\\n  ...</td>\n",
       "      <td>[def, _trim_css_to_bounds, (, css, ,, image_sh...</td>\n",
       "      <td>Make sure a tuple in (top, right, bottom, left...</td>\n",
       "      <td>[Make, sure, a, tuple, in, (, top, right, bott...</td>\n",
       "      <td>python</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        repo                              path  \\\n",
       "0  ageitgey/face_recognition  examples/face_recognition_knn.py   \n",
       "1  ageitgey/face_recognition  examples/face_recognition_knn.py   \n",
       "2  ageitgey/face_recognition  examples/face_recognition_knn.py   \n",
       "3  ageitgey/face_recognition           face_recognition/api.py   \n",
       "4  ageitgey/face_recognition           face_recognition/api.py   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://github.com/ageitgey/face_recognition/b...   \n",
       "1  https://github.com/ageitgey/face_recognition/b...   \n",
       "2  https://github.com/ageitgey/face_recognition/b...   \n",
       "3  https://github.com/ageitgey/face_recognition/b...   \n",
       "4  https://github.com/ageitgey/face_recognition/b...   \n",
       "\n",
       "                                                code  \\\n",
       "0  def train(train_dir, model_save_path=None, n_n...   \n",
       "1  def predict(X_img_path, knn_clf=None, model_pa...   \n",
       "2  def show_prediction_labels_on_image(img_path, ...   \n",
       "3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   \n",
       "4  def _trim_css_to_bounds(css, image_shape):\\n  ...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  [def, train, (, train_dir, ,, model_save_path,...   \n",
       "1  [def, predict, (, X_img_path, ,, knn_clf, =, N...   \n",
       "2  [def, show_prediction_labels_on_image, (, img_...   \n",
       "3  [def, _rect_to_css, (, rect, ), :, return, rec...   \n",
       "4  [def, _trim_css_to_bounds, (, css, ,, image_sh...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0  Trains a k-nearest neighbors classifier for fa...   \n",
       "1  Recognizes faces in given image using a traine...   \n",
       "2  Shows the face recognition results visually.\\n...   \n",
       "3  Convert a dlib 'rect' object to a plain tuple ...   \n",
       "4  Make sure a tuple in (top, right, bottom, left...   \n",
       "\n",
       "                                    docstring_tokens language partition  \n",
       "0  [Trains, a, k, -, nearest, neighbors, classifi...   python     train  \n",
       "1  [Recognizes, faces, in, given, image, using, a...   python     train  \n",
       "2  [Shows, the, face, recognition, results, visua...   python     train  \n",
       "3  [Convert, a, dlib, rect, object, to, a, plain,...   python     train  \n",
       "4  [Make, sure, a, tuple, in, (, top, right, bott...   python     train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pytrain.sample(frac=0.01, replace=False, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pytrain[\"code\"].sample(frac=0.01, replace=False, random_state=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_bpe_training(list(a))  # writing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = javatrain[\"code\"].sample(frac=0.01, replace=False, random_state=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_bpe_training(list(b))  # writing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_bpe_training(list(b), mode=\"a\")  # append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 269\n"
     ]
    }
   ],
   "source": [
    "print(len(a), len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SettingUp Software Corpora from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_bpe_text = np.load(params['saving_path']+'data_np_bpe_text.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config description: Wikipedia dataset for en, parsed from 20190301 dump.\n",
    "# Download size: 15.72 GiB\n",
    "# Dataset size: Unknown size\n",
    "# Examples: train 5,824596\n",
    "dataset_name = \"wikipedia/20200301.en\"  #'wikipedia/20190301.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-06 22:11:38,428 : INFO : Load dataset info from /root/tensorflow_datasets/wikipedia/20200301.en/1.0.0\n",
      "2020-07-06 22:11:38,444 : INFO : Reusing dataset wikipedia (/root/tensorflow_datasets/wikipedia/20200301.en/1.0.0)\n",
      "2020-07-06 22:11:38,445 : INFO : Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/wikipedia/20200301.en/1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset and create a tf.data.Dataset\n",
    "ds, info = tfds.load(dataset_name, split=\"train\", with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6033151\n"
     ]
    }
   ],
   "source": [
    "# Accessing Metadata with DatasetInfo\n",
    "print(info.splits[\"train\"].num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your input pipeline\n",
    "# ds = ds.shuffle(60000) #.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wiki = [\n",
    "    tfds.as_numpy(ex)[\"text\"].decode(\"utf-8\") for ex in ds.take(params[\"wiki_size\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_bpe_training(list_wiki, mode=\"a\")  # append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE (Byte pair encoding) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Training\n",
    "# file_bpe = params['bpe_filename']\n",
    "file_bpe = \"test_data/sentencepiece/py_java_bpe_training_c.txt\"\n",
    "# m_prefix = params['model_prefix']\n",
    "m_prefix = \"test_data/sentencepiece/py_java_bpe_32k_c\"\n",
    "text_norm = \"nfkc_cf\"  # nfkc_cf: nfkc + Unicode case folding\n",
    "# vocab_size = '128000'\n",
    "vocab_size = \"32000\"  # approx by word2vec statistics 32K\n",
    "# vocab_size = '8000'\n",
    "symbols = \"<n>,<t>,<@>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={file_bpe} --model_prefix={m_prefix} --vocab_size={vocab_size} --model_type=bpe --normalization_rule_name={text_norm}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without normalization\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={file_bpe} --model_prefix={m_prefix} --vocab_size={vocab_size} --model_type=bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this for Generators\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={file_bpe} --user_defined_symbols={symbols} --model_prefix={m_prefix} --vocab_size={vocab_size} --model_type=bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load(m_prefix + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁this', '▁is', '▁a', '▁test', '▁hello', '▁world', '▁for', '{', '▁int', '▁i', ':', '▁2', '1}']\n"
     ]
    }
   ],
   "source": [
    "print(\"*** BPE ***\")\n",
    "print(sp_bpe.encode_as_pieces(\"this is a test hello world for{ int i: 21}\"))\n",
    "# print(sp_bpe.nbest_encode_as_pieces('hello world', 5))  # returns an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁test', '▁hello', '▁world', '▁for', '{', '▁int', '▁i', ':', '▁2', '1}']\n",
      "['▁this', '▁is', '▁a', '▁test', '▁', '<n>', '▁hello', '▁world', '▁for', '{', '▁int', '▁i', ':', '▁2', '1}']\n"
     ]
    }
   ],
   "source": [
    "print(sp_bpe.encode_as_pieces(\"this is a test \\n hello world for{ int i: 21}\"))\n",
    "print(sp_bpe.encode_as_pieces(\"this is a test <n>\\n hello world for{ int i: 21}\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁test', '▁hello', '▁world', '▁for', '{', '▁int', '▁i', ':', '▁2', '1}']\n",
      "['▁this', '▁is', '▁a', '▁test', '▁', '<t>', '▁', '<t>', '▁hello', '▁world', '▁for', '{', '▁int', '▁i', ':', '▁2', '1}']\n"
     ]
    }
   ],
   "source": [
    "print(sp_bpe.encode_as_pieces(\"this is a test \\t hello world for{ int i: 21}\"))\n",
    "print(\n",
    "    sp_bpe.encode_as_pieces(\"this is a test <t>\\t\\t\\t<t> hello world for{ int i: 21}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁test', '▁', '@', '▁', '@', '▁hello', '▁world', '▁for', '{', '▁int', '▁', '@', 'i', ':', '▁2', '1}']\n",
      "['▁this', '▁is', '▁a', '▁test', '▁', '<@>', '@@', '<@>', '▁hello', '▁world', '▁for', '{', '▁int', '▁', '<@>', 'i', ':', '▁2', '1}']\n"
     ]
    }
   ],
   "source": [
    "print(sp_bpe.encode_as_pieces(\"this is a test @ @  hello world for{ int @i: 21}\"))\n",
    "print(sp_bpe.encode_as_pieces(\"this is a test <@>@@<@> hello world for{ int <@>i: 21}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207, 249, 213, 3196, 31894, 0, 31894, 0, 198, 13409, 125, 8925, 160, 31925, 161, 31894, 0, 31899, 31954, 691, 18712]\n"
     ]
    }
   ],
   "source": [
    "print(sp_bpe.encode_as_ids(\"this is a test @ @  hello world for{ int @i: 21}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁private', '▁H', 'i', '_', 'Alt', 'it', 'uted', '(', 'float', '▁j', '▁=', '▁0', '0;', '▁j', '++)']\n",
      "[423, 481, 31901, 31940, 6458, 35, 6814, 31909, 2523, 343, 15, 160, 1519, 343, 714]\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp_bpe.encode_as_pieces(\"private Hi_Altituted(float j = 00; j++)\"))\n",
    "print(sp_bpe.encode_as_ids(\"private Hi_Altituted(float j = 00; j++)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {sp_bpe.id_to_piece(id): 0 for id in range(sp_bpe.get_piece_size())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'@'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-86b25936c45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '@'"
     ]
    }
   ],
   "source": [
    "vocab[\"@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "    print(sp_bpe.id_to_piece(id), sp_bpe.is_control(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import random\n",
    "import sentencepiece as sp\n",
    "\n",
    "from fastprogress.fastprogress import master_bar\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_dfs(path):\n",
    "    \"\"\"\n",
    "    Grabs the different data splits and converts them into dataframes.\n",
    "    Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path / split).glob(\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\"])\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/tf/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protected final void bindIndexed(Configuration...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public void setServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public void setServletNames(Collection&lt;String&gt;...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void addServletNames(String... servletN...</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  protected final void bindIndexed(Configuration...   \n",
       "1  public void setServletRegistrationBeans(\\n\\t\\t...   \n",
       "2  public void addServletRegistrationBeans(\\n\\t\\t...   \n",
       "3  public void setServletNames(Collection<String>...   \n",
       "4  public void addServletNames(String... servletN...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn, df_val, df_tst = get_dfs(path / \"java/final/jsonl\")\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some test data\n",
    "df_trn.sample(frac=0.01).to_csv(\"./test_data/trn.csv\", index=False)\n",
    "df_val.sample(frac=0.01).to_csv(\"./test_data/val.csv\", index=False)\n",
    "df_tst.sample(frac=0.01).to_csv(\"./test_data/tst.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    if cols is None:\n",
    "        cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "\n",
    "    with open(output / \"text.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(list(merged_df)))\n",
    "    return output / \"text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_df(df, output, model_name, cols=None):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(\n",
    "        f\"--input={fname} --model_prefix={output / model_name} --hard_vocab_limit=false\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_glob(path, glob, model_name):\n",
    "    fns = list(path.glob(glob))\n",
    "    fns = \",\".join(map(str, fns))\n",
    "    sp.SentencePieceTrainer.train(\n",
    "        f\"--input={fns} --model_prefix={path / model_name} --hard_vocab_limit=false\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_hugface_model(\n",
    "    df,\n",
    "    output,\n",
    "    tokenizer=ByteLevelBPETokenizer(),\n",
    "    vocab_sz=30_000,\n",
    "    min_freq=3,\n",
    "    cols=None,\n",
    "):\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    tokenizer.train(\n",
    "        files=[str(fname)],\n",
    "        vocab_size=vocab_sz,\n",
    "        min_frequency=min_freq,\n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private static void createCode(String packageN...</td>\n",
       "      <td>Create the Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Override\\n    public void flushCache() {\\n   ...</td>\n",
       "      <td>LI3492-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addRule(IntDependency dependency, ...</td>\n",
       "      <td>Add this dependency with the given count to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Override\\n  public boolean removeIfEquals(K k...</td>\n",
       "      <td>Remove the object from the cache.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void marshall(DatasetContentDeliveryDes...</td>\n",
       "      <td>Marshall the given parameter object.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  private static void createCode(String packageN...   \n",
       "1  @Override\\n    public void flushCache() {\\n   ...   \n",
       "2  public void addRule(IntDependency dependency, ...   \n",
       "3  @Override\\n  public boolean removeIfEquals(K k...   \n",
       "4  public void marshall(DatasetContentDeliveryDes...   \n",
       "\n",
       "                                           docstring  \n",
       "0                                    Create the Java  \n",
       "1                                           LI3492-2  \n",
       "2  Add this dependency with the given count to th...  \n",
       "3                  Remove the object from the cache.  \n",
       "4               Marshall the given parameter object.  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path / \"trn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gen_hugface_model(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'public', 'Ġstatic', 'Ġvoid', 'Ġmain', '(', 'String', '[]', 'Ġargs', ')', 'Ġ{', 'Ġget', 'Dir', 'From', 'Lib', '();', 'Ġ}', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"public static void main(String[] args) { getDirFromLib(); }\"\n",
    "    ).tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_data/java_tokenizer-vocab.json', 'test_data/java_tokenizer-merges.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(str(path), \"java_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "    \"first\": [\"1\", \"2\", \"6\", \"7\", \"8\"],\n",
    "    \"second\": [\"K\", \"M\", \"O\", \"Q\", \"S\"],\n",
    "    \"third\": [\"L\", \"N\", \"P\", \"R\", \"T\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id Feature1 Feature2\n",
       "0  1        K        L\n",
       "1  2        M        N\n",
       "2  6        O        P\n",
       "3  7        Q        R\n",
       "4  8        S        T"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dummy_data2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('test_data/text.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_txt_file(df, Path(\"./test_data\"), list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_from_dfs(df, path, model_name, list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path / f\"{model_name}.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'Hello,', '▁', 'world!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.EncodeAsPieces(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize_fns(fns, tokenizer, exts, output, data_type):\n",
    "    docs = []\n",
    "    for fn in fns:\n",
    "        system = fn.parent.name\n",
    "        output_path = output / system / data_type\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        files = []\n",
    "        for ext in exts:\n",
    "            files.extend(fn.glob(f\"**/*.{ext}\"))\n",
    "        for file in files:\n",
    "            if \"README\" not in file.name:\n",
    "                with open(file, encoding=\"ISO-8859-1\") as f:\n",
    "                    docs.append(tokenizer.EncodeAsPieces(f.read()))\n",
    "                with open((output_path / file.name).with_suffix(\".bpe\"), \"w\") as f:\n",
    "                    f.write(\" \".join(docs[-1]))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_bpe_files(path):\n",
    "    bpe_files = []\n",
    "    for file in path.glob(\"**/*.bpe\"):\n",
    "        with open(file) as f:\n",
    "            bpe_files.append(f.read().split(\" \"))\n",
    "\n",
    "    return bpe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def split_lines_to_files(lines, fn_pattern, output_path, tokenizer):\n",
    "    for line in lines:\n",
    "        fn, content = line.split(fn_pattern)\n",
    "        fn = fn.replace('\"', \"\")\n",
    "        fn = fn.replace(\" Test \", \"\")\n",
    "        content = tokenizer.EncodeAsPieces(content)\n",
    "        with open((output_path / fn).with_suffix(\".bpe\"), \"w\") as f:\n",
    "            f.write(\" \".join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../benchmarking/traceability/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path / \"datasets/italian/italian_bpe.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../benchmarking/traceability/datasets/italian/ebt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebt_path = path / \"datasets/italian/ebt\"\n",
    "ebt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path / \"[ebt-raw-req].txt\") as f:\n",
    "    split_lines_to_files(\n",
    "        f.read().split(\"\\n\")[:-1], \"\\t\", path / \"testbeds/bpe/italian/ebt/req\", spm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path / \"[ebt-raw-tc].txt\") as f:\n",
    "    split_lines_to_files(\n",
    "        f.read().split(\"\\n\")[:-1], \"case:\", path / \"testbeds/bpe/italian/ebt/tc\", spm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ground_truth(path, language):\n",
    "    all_links = pd.DataFrame(\n",
    "        [],\n",
    "        columns=[\n",
    "            \"sys\",\n",
    "            \"from_type\",\n",
    "            \"to_type\",\n",
    "            \"from_file\",\n",
    "            \"to_file\",\n",
    "            \"from_doc\",\n",
    "            \"to_doc\",\n",
    "        ],\n",
    "    )\n",
    "    for fn in path.glob(\"*.txt\"):\n",
    "        content = str(fn.name).split(\".\")[0][1:-1]\n",
    "        content = content.split(\"-\")\n",
    "\n",
    "        sys, from_type, to_type = content[0], content[2], content[4]\n",
    "\n",
    "        with open(fn) as f:\n",
    "            links = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "        for link in links:\n",
    "            link = link.split(\" \")\n",
    "            root, children = link[0], link[1:]\n",
    "            root = Path(root).with_suffix(\".bpe\").name\n",
    "            with open(\n",
    "                path.parent.parent / \"bpe\" / language / sys / from_type / root\n",
    "            ) as f:\n",
    "                root_content = f.read().split(\" \")\n",
    "            children = [Path(child).with_suffix(\".bpe\").name for child in children]\n",
    "            children = [\n",
    "                Path(\".\".join(str(child).split(\".\")[-2:])) for child in children\n",
    "            ]\n",
    "            for child in children:\n",
    "                with open(\n",
    "                    path.parent.parent / \"bpe\" / language / sys / to_type / child\n",
    "                ) as f:\n",
    "                    child_content = f.read().split(\" \")\n",
    "                all_links = all_links.append(\n",
    "                    {\n",
    "                        \"sys\": sys,\n",
    "                        \"from_type\": from_type,\n",
    "                        \"to_type\": to_type,\n",
    "                        \"from_file\": root,\n",
    "                        \"to_file\": str(child),\n",
    "                        \"from_doc\": root_content,\n",
    "                        \"to_doc\": child_content,\n",
    "                    },\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_non_ground_truth(path, language, gt):\n",
    "    all_non_links = []\n",
    "\n",
    "    existing_links = [\n",
    "        \"->\".join(link)\n",
    "        for link in zip(gt[\"from_file\"].to_list(), gt[\"to_file\"].to_list())\n",
    "    ]\n",
    "    bpe_files = list(path.glob(\"**/*.bpe\"))\n",
    "    random.shuffle(bpe_files)\n",
    "    for i in bpe_files[:500]:\n",
    "        sys = i.parent.parent.name\n",
    "        from_type = i.parent.name\n",
    "        if str(from_type) != \"req\":\n",
    "            continue\n",
    "        with open(i) as f:\n",
    "            i_content = f.read().split(\" \")\n",
    "        random.shuffle(bpe_files)\n",
    "        for j in bpe_files[:500]:\n",
    "            if i == j:\n",
    "                continue\n",
    "            if \"->\".join([i.name, j.name]) in existing_links:\n",
    "                continue\n",
    "            to_type = j.parent.name\n",
    "            if str(to_type) == \"req\":\n",
    "                continue\n",
    "            #             if from_type == to_type: continue\n",
    "            with open(j) as f:\n",
    "                j_content = f.read().split(\" \")\n",
    "            all_non_links.append(\n",
    "                [sys, from_type, to_type, i.name, j.name, i_content, j_content]\n",
    "            )\n",
    "\n",
    "    all_non_links = pd.DataFrame(\n",
    "        all_non_links,\n",
    "        columns=[\n",
    "            \"sys\",\n",
    "            \"from_type\",\n",
    "            \"to_type\",\n",
    "            \"from_file\",\n",
    "            \"to_file\",\n",
    "            \"from_doc\",\n",
    "            \"to_doc\",\n",
    "        ],\n",
    "    )\n",
    "    return all_non_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_gt_ngt(path, lang):\n",
    "    gt = get_ground_truth(path / \"groundtruth\" / lang, lang)\n",
    "    ngt = get_non_ground_truth(path / \"bpe\" / lang, lang, gt)\n",
    "\n",
    "    return gt, ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_mgmnt.prep.i.ipynb.\n",
      "Converted 01_exp.i.ipynb.\n",
      "Converted 02_mgmnt.db.mongo.ipynb.\n",
      "Converted 03_repr.i.ipynb.\n",
      "Converted 04_mining.ir.model.ipynb.\n",
      "Converted 05_mining.ir.i.ipynb.\n",
      "Converted 06_benchmark.traceability.ipynb.\n",
      "Converted 07_repr.roberta.train.ipynb.\n",
      "Converted 08_exp.info.ipynb.\n",
      "Converted 09_desc.stats.ipynb.\n",
      "Converted 10_vis.ipynb.\n",
      "Converted 11_mgmnt.prep.nltk.ipynb.\n",
      "Converted 12_repr.roberta.eval.ipynb.\n",
      "Converted 14_mgmnt.prep.bpe.ipynb.\n",
      "Converted 15_desc.metrics.se.ipynb.\n",
      "Converted 16_repr.word2vec.train.ipynb.\n",
      "Converted 17_repr.doc2vec.train.ipynb.\n",
      "Converted 18_repr.doc2vec.eval.ipynb.\n",
      "Converted 19_repr.word2vec.eval.ipynb.\n",
      "Converted 20_benchmark.codegen.ipynb.\n",
      "Converted 21_inf.i.ipynb.\n",
      "Converted 22_inf.bayesian.ipynb.\n",
      "Converted 23_inf.causal.ipynb.\n",
      "Converted aa_blog.example.ipynb.\n",
      "Converted ab_templates.example.ipynb.\n",
      "Converted ac_emp.eval.pp1.rq1.ipynb.\n",
      "Converted ad_emp.eval.pp1.rq2.ipynb.\n",
      "Converted ae_emp.eval.pp1.rq3.ipynb.\n",
      "Converted af_emp.eval.pp1.rq4.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

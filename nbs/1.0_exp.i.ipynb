{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exp.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import dit\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import sem, t\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVED NECESSARY PIECES TO 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual mongo call\n",
    "def simulate_getting_dataframes_from_mongo():\n",
    "    corpus_data = {'file_name': [], 'data_type': [], 'contents': []}\n",
    "    path = \"./test_data/LibEST_semeru_format/requirements\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('req')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/source_code\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('src')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/test\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('test')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Form does not differ from the void, and the vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Form is the void, and the void is form.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents\n",
       "0  Form does not differ from the void, and the vo...\n",
       "1            Form is the void, and the void is form."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hide\n",
    "#This isn't actually used so we don't need to test it but here you can see how a data frame is organized\n",
    "#df = simulate_getting_dataframes_from_mongo()\n",
    "datas = [\"Form does not differ from the void, and the void does not differ from the form.\", \"Form is the void, and the void is form.\"]\n",
    "df = pd.DataFrame(data=datas,columns=['contents'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "  \n",
    "    with open(output + '_text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output + '_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_to_txt_file(df,'alexTest', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Create a file\n",
    "df_to_txt_file(df,'output', None)\n",
    "#Is it there?\n",
    "test_bool = os.path.isfile('./output_text.txt')\n",
    "assert(test_bool == True)\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gen_sp_model(df, output, model_name, cols=None):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
    "    return output + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Here we test the gen sp model method to see if it properly creates files\n",
    "actual = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "expected = os.path.isfile('./' + actual + '.model')\n",
    "assert(expected)\n",
    "os.remove('./' + actual + '.model')\n",
    "expected2 = os.path.isfile('./'+ actual + '.vocab')\n",
    "assert(expected2)\n",
    "os.remove('./'+ actual + '.vocab')\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_text(text, model_prefix):\n",
    "    '''Encodes text using a pre-trained sp model, returns the occurrences of each token in the text'''\n",
    "    sp_processor = sp.SentencePieceProcessor()\n",
    "    sp_processor.Load(f\"{model_prefix}.model\")\n",
    "    token_counts = Counter()\n",
    "    encoding = sp_processor.encode_as_pieces(text)\n",
    "    for piece in encoding:\n",
    "        token_counts[piece] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "\n",
    "#This tests the encode text method assuming the sp model(?)\n",
    "\n",
    "sp_model = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "\n",
    "actual = encode_text(\"Form does not differ from the void, and the void does not differ from the form.\", 'output_sp_bpe_modal')\n",
    "\n",
    "expected = Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n",
    "#Clean up Test\n",
    "assert(actual == expected)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Notes: token_counts is a counter object.\n",
    "def dit_shannon(token_counts):\n",
    "    '''Takes in a counter object of token occurrences, computes the entropy of the corpus that produced it'''\n",
    "    num_tokens = 0\n",
    "    for token in token_counts:\n",
    "        num_tokens += token_counts[token]\n",
    "    outcomes = list(set(token_counts.elements()))\n",
    "    frequencies = []\n",
    "    for token in token_counts:\n",
    "        frequencies.append((token_counts[token])/num_tokens)\n",
    "    d = dit.ScalarDistribution(outcomes, frequencies)\n",
    "    return dit.shannon.entropy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6783847516036627\n",
      "3.6783847516036627\n",
      "Counter({' ': 15, 'o': 10, 'd': 7, 'e': 7, 'f': 7, 'r': 6, 't': 5, 'm': 4, 'i': 4, 'n': 3, 'h': 3, 's': 2, 'v': 2, 'F': 1, ',': 1, 'a': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "#This test takes a sample string and creates a counter that it runs dit shannon on to verify the shannon functionality\n",
    "sample_txt = \"Form does not differ from the void, and the void does not differ from the form.\"\n",
    "sample_tcnt = Counter(sample_txt)\n",
    "\n",
    "answer = dit_shannon(sample_tcnt)\n",
    "expected = 3.6783847516036627\n",
    "#assert(answer == expected)\n",
    "print(answer)\n",
    "print(expected)\n",
    "print(sample_tcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def entropies_of_df_entries(df, col, model_prefix):\n",
    "    '''Returns a list of the entropies of each entry in a dataframe column'''\n",
    "    entropies = []\n",
    "    for data in df[col]:\n",
    "        token_counts= encode_text(data, model_prefix)\n",
    "        entropies.append(dit_shannon(token_counts))\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "#Entropies of df test\n",
    "enty = entropies_of_df_entries(df,'contents','output_sp_bpe_modal')\n",
    "\n",
    "entropy1 = dit_shannon(Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, \n",
    "                                '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "entropy2 = dit_shannon(Counter({'▁is': 2, '▁the': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "assert(enty == [entropy1, entropy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish this such that is finds the entropy of the entire corpus \\\n",
    "#       and preserves the individual token frequencies so that we can   \\\n",
    "#      compute the most common tokens\n",
    "\n",
    "# def entropy_of_whole_corpus(df, col, model_prefix):\n",
    "#     '''Returns a dictionary of the entropies of each token in a dataframe corpus'''\n",
    "#     entropies = {}\n",
    "#     token_counts = encode_text(pd.concat[col], model_prefix)\n",
    "#     entropies.append(dit_shannon(token_counts))\n",
    "#     return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "import math\n",
    "def manual_shannon(token_freqs):\n",
    "    sum = 0\n",
    "    for i in token_freqs:\n",
    "        sum += i * math.log(1/i, 2)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "def sort_token_data(token_data):\n",
    "    return sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBest Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide\n",
    "# Create a dataframe of the requirements, source code and test case data for LIBest\n",
    "# Create a sentencepiece model using the entire LIBest corpus\n",
    "LIB_corpus_df = simulate_getting_dataframes_from_mongo()\n",
    "LIB_model = gen_sp_model(LIB_corpus_df, output='LIBest', model_name='_sp_bpe_modal', cols=['contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide\n",
    "# Use the model to compute each file's entropy\n",
    "LIB_entropies = entropies_of_df_entries(LIB_corpus_df, 'contents', LIB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max entropy: 8.736686724450998\n",
      "Min entropy: 3.979797585487148\n",
      "Average entropy: 6.9103334297633525\n",
      "Median entropy: 7.086176924292342\n",
      "Entropy Standard Deviation: 1.069417835431708\n",
      "95% of entropies fall within 6.681088254680821 and 7.139578604845884\n"
     ]
    }
   ],
   "source": [
    "#Hide\n",
    "# Calculate metrics on the LIBest corpus entropies\n",
    "#print(\"Max entropy:\", max(LIB_entropies))\n",
    "#print(\"Min entropy:\", min(LIB_entropies))\n",
    "#print(\"Average entropy:\", mean(LIB_entropies))\n",
    "#print(\"Median entropy:\", stat.median(LIB_entropies))\n",
    "\n",
    "#print(\"Entropy Standard Deviation:\", std(LIB_entropies))\n",
    "\n",
    "#confidence = 0.95\n",
    "#n = len(LIB_entropies)\n",
    "#m = mean(LIB_entropies)\n",
    "#std_err = sem(LIB_entropies)\n",
    "#h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "#start = m - h\n",
    "#end = m + h\n",
    "#print(f\"95% of entropies fall within {start} and {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hide\n",
    "# Create a histogram of the entropy distribution\n",
    "#plt.hist(LIB_entropies, bins = 20)\n",
    "#plt.ylabel(\"Num Files\")\n",
    "#plt.xlabel(\"Entropy\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Corpus as a Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

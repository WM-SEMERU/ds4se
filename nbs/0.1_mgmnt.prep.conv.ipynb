{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Preprocessing\n",
    "\n",
    "> This module comprises preprocessing techniques applied to software artifacts (TODO:cite here the papers employed for this preprocessings):\n",
    ">\n",
    ">This is an adapted version of Daniel McCrystal Nov 2019\n",
    ">\n",
    ">This version also includes BPE preprocesing and NLTK. It's the main class to execute conventional pipelines. \n",
    "\n",
    ">Author: @danaderp March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install dit\n",
    "#! pip install nltk\n",
    "#! pip install tokenizers\n",
    "#! pip install tensorflow_datasets\n",
    "! pip install -U tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List, Set, Callable, Tuple, Dict, Optional\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from string import punctuation\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import sentencepiece as spm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq cisco/CSB-CICDPipelineEdition-master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def libest_params():\n",
    "    return {\n",
    "        'system': 'libest',\n",
    "        #'path_zip': Path(\"cisco/sacp-python-common.zip\"),\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/libest_data',\n",
    "        'language': 'english',\n",
    "        'dataset' : path_data + ''\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k' #For BPE Analysis\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k'\n",
    "        'model_prefix':path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = {\n",
    "    'bpe8k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k',\n",
    "    'bpe32k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k',\n",
    "    'bpe128k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = default_params()\n",
    "params = libest_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConventionalPreprocessing():\n",
    "    '''NLTK libraries for Conventional Preprocessing'''\n",
    "    def __init__(self, params, bpe = False):\n",
    "        self.params = params\n",
    "        \n",
    "        #If BPE provided, then preprocessing with BPE is allowed on CONV\n",
    "        if bpe:\n",
    "            self.sp_bpe = spm.SentencePieceProcessor()\n",
    "            self.sp_bpe.load(params['model_prefix']+'.model')\n",
    "        else:\n",
    "            self.sp_bpe = None\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def bpe_pieces_pipeline(self, doc_list):\n",
    "        '''Computes BPE preprocessing according to params'''\n",
    "        encoded_str = ''\n",
    "        if self.sp_bpe is None:\n",
    "            logging.info('Provide a BPE Model!')\n",
    "        else:\n",
    "            encoded_str = [self.sp_bpe.encode_as_pieces(doc) for doc in doc_list]  \n",
    "        return encoded_str\n",
    "    \n",
    "    #ToDo Transforme it into a For-Comprenhension\n",
    "    def clean_punctuation(self, token): \n",
    "        #remove terms !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789\n",
    "        return re.sub(r'[^a-zA-Z\\s]', ' ', token, re.I|re.A)\n",
    "\n",
    "    def split_camel_case_token(self, token):\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token)\n",
    "\n",
    "    def remove_terms(self, filtered_tokens):\n",
    "        remove_terms = punctuation + '0123456789'\n",
    "        return [token for token in filtered_tokens if token not in remove_terms and len(token)>2 and len(token)<21]\n",
    "\n",
    "    def stemmer(self, filtered_tokens):\n",
    "        return [englishStemmer.stem(token) for token in filtered_tokens ]\n",
    "\n",
    "    def stop_words(self, filtered_tokens):\n",
    "        stop_words = nltk.corpus.stopwords.words(self.params['language'])\n",
    "        return [token for token in filtered_tokens if token not in stop_words]\n",
    "    \n",
    "    def basic_pipeline(self, dict_filenames):\n",
    "        '''@dict_filenames: {filename: code}'''\n",
    "        pre_process = [( key.replace('.txt', '-pre.txt') , self.clean_punctuation(dict_filenames[key][0])  ) for key in dict_filenames]\n",
    "        pre_process = [( doc[0] , self.split_camel_case_token(doc[1])  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].lower()  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].strip()) for doc in pre_process] # Leading whitepsace are removed\n",
    "        pre_process_tokens = [(doc[0] , nltk.WordPunctTokenizer().tokenize(doc[1])) for doc in pre_process]\n",
    "        filtered_tokens = [(doc[0], self.stop_words(doc[1]) ) for doc in pre_process_tokens] #Stop Words\n",
    "        filtered_tokens = [(doc[0], self.stemmer(doc[1]) ) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        filtered_tokens = [(doc[0], self.remove_terms(doc[1])) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        pre_process = [(doc[0], ' '.join(doc[1])) for doc in filtered_tokens]\n",
    "        return pre_process\n",
    "    \n",
    "    def fromdocs_pipeline(self, docs):\n",
    "        #TODO\n",
    "        \"\"\"@tokenized_file: a list of tokens that represents a document/code\"\"\"\n",
    "        pre_process = [ self.clean_punctuation(doc) for doc in docs]\n",
    "        logging.info('fromtokens_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('fromtokens_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('fromtokens_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('fromtokens_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('fromtokens_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('fromtokens_pipeline: Removed Special Terns')\n",
    "        pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def frombatch_pipeline(self, batch):\n",
    "        #TODO\n",
    "        \"\"\"@batch: a TensorFlow Dataset Batch\"\"\"\n",
    "        pre_process = [ self.clean_punctuation( doc.decode(\"utf-8\") ) for doc in batch]\n",
    "        logging.info('frombatch_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('frombatch_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('frombatch_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('frombatch_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('frombatch_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('frombatch_pipeline: Removed Special Terns')\n",
    "        #pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('frombatch_pipeline [END]')\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fromtensor_pipeline(self, ts_x):\n",
    "        \"\"\"@ts_x: es un elemento del tensor\"\"\"\n",
    "        #TODO\n",
    "        pre_process = self.clean_punctuation(ts_x)\n",
    "        pre_process = self.split_camel_case_token(pre_process)\n",
    "        pre_process = pre_process.lower()\n",
    "        pre_process = pre_process.strip()\n",
    "        pre_process = nltk.WordPunctTokenizer().tokenize(pre_process)\n",
    "        filtered_tokens = self.stop_words(pre_process)\n",
    "        filtered_tokens = self.stemmer(filtered_tokens)\n",
    "        filtered_tokens = self.remove_terms(filtered_tokens)\n",
    "        pre_process = ' '.join(filtered_tokens)\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def SaveCorpus(self, df, language='js', sep=',', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "\n",
    "        df.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)     \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def LoadCorpus(self, timestamp, language='js', sep=',', mode='a'):\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "        return pd.read_csv(path_to_link, header=0, index_col=0, sep=sep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def open_file(f, encoding='utf-8'):\n",
    "    try:\n",
    "        #return open(filename, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        return open(f, 'r', encoding = encoding).read()\n",
    "    except:\n",
    "        print(\"Exception: \", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_files(system, ends):\n",
    "    path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "    names = [entry for entry in path.glob('**/*' +ends)]\n",
    "    filenames = [(filename, os.path.basename(filename), open_file(filename) ) for filename in names]\n",
    "    return pd.DataFrame( filenames ,columns = ['names','filenames','content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing Software Corpora from GitHub\n",
    "> Cisco Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def sacp_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'sacp-python-common',\n",
    "        'path_zip': Path(\"tf/data/cisco/sacp_data/sacp-python-common.zip\"),\n",
    "        'dataset': 'tf/data/cisco/sacp_data/',\n",
    "        'saving_path': '../../'+'data/cisco/sacp_data/',\n",
    "        'language': 'english',\n",
    "        'model_prefix':model_prefix #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'sacp-python-common',\n",
       " 'path_zip': PosixPath('tf/data/cisco/sacp_data/sacp-python-common.zip'),\n",
       " 'dataset': 'tf/data/cisco/sacp_data/',\n",
       " 'saving_path': '../../data/cisco/sacp_data/',\n",
       " 'language': 'english',\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_8k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = sacp_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ConventionalPreprocessing(params, bpe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore\n",
    "archive = ZipFile(Path(\"cisco/sacp-python-common.zip\"), 'r')\n",
    "files = archive.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore\n",
    "files = [name for name in archive.namelist() if name.endswith('.py')] #recursively finds files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_file_zip(params, ends):\n",
    "    archive = ZipFile( params['path_zip'], 'r')\n",
    "    names = [name for name in archive.namelist() if name.endswith(ends)]\n",
    "    filenames = [(filename, os.path.basename(filename), archive.read(filename) ) for filename in names]\n",
    "    return pd.DataFrame( filenames ,columns = ['names','filenames','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling = get_file_zip(params = params, ends='.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "prep.SaveCorpus(df_sampling, language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling = prep.LoadCorpus(1595859280.080238, language='py')\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating standard dataframe for issues and pull-request (cisco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data = pd.read_csv('cisco/sacp-pullrequest-01.csv', sep = '~', header = 0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the system artifacts in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys = pr_data.copy()\n",
    "pr_all_sys = pr_all_sys.replace(np.nan, ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys['text'] = pr_all_sys['title'].astype(str) + pr_all_sys['labels'].astype(str) + pr_all_sys['body'].astype(str)#merging tree columns for the text\n",
    "pr_all_sys = pr_all_sys[['id-pr','text']]\n",
    "pr_all_sys = pr_all_sys.rename(columns={'id-pr': 'ids'})\n",
    "pr_all_sys['type'] = 'pr' #<------- File Type Standard for Target or Source\n",
    "pr_all_sys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_code = df_sampling.copy()\n",
    "#pr_all_code['text'] =  pr_all_code.apply(lambda row: row['content'].decode(\"utf-8\"), axis = 1)\n",
    "pr_all_code['content'] =  pr_all_code['content'].apply(lambda x: eval(x))\n",
    "pr_all_code['text'] =  pr_all_code['content'].apply(lambda x: x.decode(\"utf-8\"))\n",
    "pr_all_code = pr_all_code[['names','text']]\n",
    "pr_all_code = pr_all_code.rename(columns={'names': 'ids'})\n",
    "pr_all_code['type'] = 'py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys = pd.concat([pr_all_sys, pr_all_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys['conv'] = prep.fromdocs_pipeline( pr_all_sys['text'].values ) #Conventional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.SaveCorpus(pr_all_sys, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacp = prep.LoadCorpus(1595953540.866044, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>conv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295</td>\n",
       "      <td>Production Merge * Feed release name through t...</td>\n",
       "      <td>pr</td>\n",
       "      <td>product merg feed releas name upload bom allow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294</td>\n",
       "      <td>Add test fields for DARE push * Added test dat...</td>\n",
       "      <td>pr</td>\n",
       "      <td>add test field dare push test data json sent d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293</td>\n",
       "      <td>Allow passing a release to uploadBom by name, ...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow pass releas upload bom name rather chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287</td>\n",
       "      <td>Allow append images #363 - Changed how image n...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow append imag chang imag name creat send c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274</td>\n",
       "      <td>Move docker/blackduck test to slave 4</td>\n",
       "      <td>pr</td>\n",
       "      <td>move docker blackduck test slave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text type  \\\n",
       "0  295  Production Merge * Feed release name through t...   pr   \n",
       "1  294  Add test fields for DARE push * Added test dat...   pr   \n",
       "2  293  Allow passing a release to uploadBom by name, ...   pr   \n",
       "3  287  Allow append images #363 - Changed how image n...   pr   \n",
       "4  274            Move docker/blackduck test to slave 4     pr   \n",
       "\n",
       "                                                conv  \n",
       "0  product merg feed releas name upload bom allow...  \n",
       "1  add test field dare push test data json sent d...  \n",
       "2  allow pass releas upload bom name rather chang...  \n",
       "3  allow append imag chang imag name creat send c...  \n",
       "4                   move docker blackduck test slave  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sacp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacp.dropna( inplace = True ) #empty files are not considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating All Possible BPEs configs\n",
    "for bpe in model_prefix.keys():\n",
    "    mpr = model_prefix[bpe]\n",
    "    prep = ConventionalPreprocessing(sacp_params(model_prefix = mpr), bpe = True)\n",
    "    df_sacp[bpe] = prep.bpe_pieces_pipeline( df_sacp['text'].values ) #BPE Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>conv</th>\n",
       "      <th>bpe8k</th>\n",
       "      <th>bpe32k</th>\n",
       "      <th>bpe128k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295</td>\n",
       "      <td>Production Merge * Feed release name through t...</td>\n",
       "      <td>pr</td>\n",
       "      <td>product merg feed releas name upload bom allow...</td>\n",
       "      <td>[▁production, ▁mer, ge, ▁*, ▁feed, ▁release, ▁...</td>\n",
       "      <td>[▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...</td>\n",
       "      <td>[▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294</td>\n",
       "      <td>Add test fields for DARE push * Added test dat...</td>\n",
       "      <td>pr</td>\n",
       "      <td>add test field dare push test data json sent d...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁d, are, ▁p, ush,...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293</td>\n",
       "      <td>Allow passing a release to uploadBom by name, ...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow pass releas upload bom name rather chang...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁upload,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287</td>\n",
       "      <td>Allow append images #363 - Changed how image n...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow append imag chang imag name creat send c...</td>\n",
       "      <td>[▁allow, ▁app, end, ▁images, ▁#, 3, 63, ▁-, ▁c...</td>\n",
       "      <td>[▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...</td>\n",
       "      <td>[▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274</td>\n",
       "      <td>Move docker/blackduck test to slave 4</td>\n",
       "      <td>pr</td>\n",
       "      <td>move docker blackduck test slave</td>\n",
       "      <td>[▁move, ▁d, ock, er, /, black, d, uck, ▁test, ...</td>\n",
       "      <td>[▁move, ▁dock, er, /, black, d, uck, ▁test, ▁t...</td>\n",
       "      <td>[▁move, ▁docker, /, black, duck, ▁test, ▁to, ▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text type  \\\n",
       "0  295  Production Merge * Feed release name through t...   pr   \n",
       "1  294  Add test fields for DARE push * Added test dat...   pr   \n",
       "2  293  Allow passing a release to uploadBom by name, ...   pr   \n",
       "3  287  Allow append images #363 - Changed how image n...   pr   \n",
       "4  274            Move docker/blackduck test to slave 4     pr   \n",
       "\n",
       "                                                conv  \\\n",
       "0  product merg feed releas name upload bom allow...   \n",
       "1  add test field dare push test data json sent d...   \n",
       "2  allow pass releas upload bom name rather chang...   \n",
       "3  allow append imag chang imag name creat send c...   \n",
       "4                   move docker blackduck test slave   \n",
       "\n",
       "                                               bpe8k  \\\n",
       "0  [▁production, ▁mer, ge, ▁*, ▁feed, ▁release, ▁...   \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁d, are, ▁p, ush,...   \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...   \n",
       "3  [▁allow, ▁app, end, ▁images, ▁#, 3, 63, ▁-, ▁c...   \n",
       "4  [▁move, ▁d, ock, er, /, black, d, uck, ▁test, ...   \n",
       "\n",
       "                                              bpe32k  \\\n",
       "0  [▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...   \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...   \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...   \n",
       "3  [▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...   \n",
       "4  [▁move, ▁dock, er, /, black, d, uck, ▁test, ▁t...   \n",
       "\n",
       "                                             bpe128k  \n",
       "0  [▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...  \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...  \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁upload,...  \n",
       "3  [▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...  \n",
       "4  [▁move, ▁docker, /, black, duck, ▁test, ▁to, ▁...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sacp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 06:52:58,801 : INFO : Saving in...../../data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv\n"
     ]
    }
   ],
   "source": [
    "prep.SaveCorpus(df_sacp, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code down [becareful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "names = [entry for entry in path.glob('**/*.py')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#here looking for a file with encoding problems\n",
    "temp_list=[]\n",
    "for filename in names:\n",
    "    print(filename)\n",
    "    try:\n",
    "        temp_list.append(open(filename, 'r', encoding=\"ISO-8859-1\").read())\n",
    "    except FileNotFoundError as err:\n",
    "        print('lookattheerr' + str(err))\n",
    "    except:\n",
    "        print('bydefault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java = get_files(system = params['system'], ends='.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveCorpus(df_java, language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = LoadCorpus(1592266849.29903,language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing Software Corpora from CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeSearchNet Parameters\n",
    "params = {\n",
    "    'system':'codesearchnet',\n",
    "    'saving_path': 'test_data/',\n",
    "    'language': 'english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1] Create Preprocesser <----------\n",
    "preprocess_pipeline = ConventionalPreprocessing(params= params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files = sorted(Path('codesearch/python/').glob('**/*.gz'))\n",
    "java_files = sorted(Path('codesearch/java/').glob('**/*.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_long_list = ['repo', 'path', 'url', 'code', \n",
    "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
    "                     'language', 'partition']\n",
    "\n",
    "columns_short_list = ['code_tokens', 'docstring_tokens', \n",
    "                      'language', 'partition']\n",
    "\n",
    "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, \n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_searchnet_df = jsonl_list_to_dataframe(python_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df = jsonl_list_to_dataframe(java_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain = java_searchnet_df[java_searchnet_df.partition.eq('train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javatrain = java_searchnet_df[java_searchnet_df.partition.eq('train')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javatrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.SaveCorpus(javatrain, language='java') #Saving codesearchnet only training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Preprocessing for CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df = preprocess_pipeline.LoadCorpus(1592409554.097457, language='java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df['code'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_pipeline.fromdocs_pipeline(java_searchnet_df['code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = java_searchnet_df.copy()\n",
    "df_preprocessed['preprocessed'] = preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveCorpus(df_preprocessed, language='preprocessed-java') #Saving codesearchnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing from Wikipedia \n",
    ">Inspired by [KD](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)\n",
    ">\n",
    ">Dump Wiki File [here](https://dumps.wikimedia.org/enwiki/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config description: Wikipedia dataset for en, parsed from 20190301 dump.\n",
    "#Download size: 15.72 GiB\n",
    "#Dataset size: Unknown size\n",
    "#Examples: train 5,824596\n",
    "dataset_name = 'wikipedia/20200301.en' #'wikipedia/20190301.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the dataset and create a tf.data.Dataset\n",
    "ds, info = tfds.load(dataset_name, split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing Metadata with DatasetInfo\n",
    "print(info.splits['train'].num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki = []\n",
    "#dataset_wiki = ds.map(lambda ex_text, ex_title: preprocess_pipeline.fromtensor_pipeline( ex_text.decode(\"utf-8\") ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki = [preprocess_pipeline.fromtensor_pipeline( ex['text'].decode(\"utf-8\") ) for ex in  tfds.as_numpy(ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_wiki = pd.DataFrame( dataset_wiki ,columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your input pipeline\n",
    "ds = ds.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Numpy Arrays\n",
    "for ex in tfds.as_numpy(ds):\n",
    "    #print( preprocess_pipeline.fromtensor_pipeline( ex['text'].decode(\"utf-8\") ) )\n",
    "    #print(\"NEXT!!!\")\n",
    "    #print(ex['text'].decode(\"utf-8\"))\n",
    "    #print(ex)\n",
    "    #np_text, np_title = ex['text'], ex['title']\n",
    "    print(preprocess_pipeline.frombatch_pipeline( ex['text'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_text[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in ds.take(4):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'system':'wiki',\n",
    "    'saving_path': 'test_data/',\n",
    "    'language': 'english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Processing from Semeru Format and Converting into Mappings\n",
    "> @danaderp July 29'20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "semeru_format =  path_data + 'se-benchmarking/traceability/datasets/formatted/semeru_format/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Up SemeruFormat\n",
    "def libest_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'libest',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'LibEST_semeru_format/requirements'),\n",
    "            'src':pathlib.Path( semeru_format + 'LibEST_semeru_format/source_code'),\n",
    "            'tc':pathlib.Path( semeru_format + 'LibEST_semeru_format/test')\n",
    "        },\n",
    "        'ends': ['.txt','.c','.h'],\n",
    "        'model_prefix':model_prefix,\n",
    "        'encoding':'utf-8'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebt_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'ebt',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'EBT_semeru_format/requirements'),\n",
    "            'tc': pathlib.Path( semeru_format + 'EBT_semeru_format/test_cases'),\n",
    "            'src':pathlib.Path( semeru_format + 'EBT_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT'],\n",
    "        'model_prefix':model_prefix,\n",
    "        #'encoding':'ISO-8859-1'\n",
    "        'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itrust_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'itrust',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'uc':pathlib.Path( semeru_format + 'iTrust_semeru_format/use_cases'),\n",
    "            'src':pathlib.Path( semeru_format + 'iTrust_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT','.jsp'],\n",
    "        'model_prefix':model_prefix,\n",
    "        'encoding':'ISO-8859-1'\n",
    "        #'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smos_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'smos',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'italian',\n",
    "        'dataset' : {\n",
    "            'uc':pathlib.Path( semeru_format + 'SMOS_semeru_format/use_cases'),\n",
    "            'src':pathlib.Path( semeru_format + 'SMOS_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT','.jsp'],\n",
    "        'model_prefix':model_prefix,\n",
    "        'encoding':'ISO-8859-1'\n",
    "        #'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'smos',\n",
       " 'saving_path': '../dvc-ds4se/se-benchmarking/traceability/testbeds/processed/',\n",
       " 'language': 'italian',\n",
       " 'dataset': {'uc': PosixPath('../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/use_cases'),\n",
       "  'src': PosixPath('../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/source_code')},\n",
       " 'ends': ['.txt', '.java', '.c', '.h', '.TXT', '.jsp'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_8k',\n",
       " 'encoding': 'ISO-8859-1'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters = libest_params(model_prefix=model_prefix['bpe8k'])\n",
    "parameters = smos_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uc', 'src'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['dataset'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-11 23:50:05,499 : INFO : artifacts: dict_keys(['uc', 'src'])\n",
      "2021-01-11 23:50:05,500 : INFO : artifacts: ../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/use_cases\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"artifacts: \"  +  str(parameters['dataset'].keys()) )\n",
    "logging.info(\"artifacts: \"  +  str(parameters['dataset']['uc'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/use_cases/SMOS37.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [entry for entry in parameters['dataset']['uc'].glob('**/*' + \".txt\" )]\n",
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [(filename, os.path.basename(filename), open_file(filename, encoding=parameters['encoding']) ) for filename in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/use_cases/SMOS37.txt'),\n",
       " 'SMOS37.txt',\n",
       " \"Nome: InserisciNota\\nAttori: Amministratore\\nDescrizione: Inserimento Note Disciplinari\\nPrecondizioni:\\nâ\\x80¢ â\\x80¢ â\\x80¢ â\\x80¢\\nL'utente deve essere loggato al sistema come Amministratore L'utente ha svolto il caso d'uso â\\x80\\x9cVisualizzaDettagliSingoloRegistroâ\\x80\\x9d L'utente ha giÃ\\xa0 svolto il caso d'uso â\\x80\\x9cVisualizzaElencoNoteâ\\x80\\x9d L'utente clicca sul pulsante â\\x80\\x9cNuova notaâ\\x80\\x9d\\nSequenza degli eventi\\nUtente\\nSistema\\n2. Compila il form 3. Clicca su â\\x80\\x9cSalvaâ\\x80\\x9d\\n1. Mostra un form con i campi della nota (studente, data, docente , descrizione).\\n4. Salva la nota e invia una notifica via e-mail al genitore\\nPostcondizioni:\\nâ\\x80¢\\nâ\\x80¢ â\\x80¢\\nI dati della nota sono stati inseriti nel sistema, ed il sistema ha inviato la notifica ai genitori. Il sistema ritorna alla schermata del registro. Lâ\\x80\\x99amministratore interrompe lâ\\x80\\x99operazione Connessione al server SMOS interrotta\")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def loading_artifacts( params ):\n",
    "    #Creating the mappings\n",
    "    \n",
    "    df_sys_g = pd.DataFrame( [] ,columns = ['ids','filenames','text']) #global dataframe\n",
    "    \n",
    "    for art in parameters['dataset'].keys():\n",
    "        sys_names = [[entry for entry in parameters['dataset'][art].glob('**/*' + ex )] for ex in parameters['ends']]\n",
    "        sys_names = functools.reduce(lambda a,b : a+b,sys_names) #Flatting\n",
    "        logging.info(\"artifacts: \"  +  str( len(sys_names) ) )\n",
    "        sys_filenames = [(filename, os.path.basename(filename), open_file(filename, encoding=params['encoding']) ) for filename in sys_names]\n",
    "        df_sys_l = pd.DataFrame( sys_filenames ,columns = ['ids','filenames','text']) #local dataframe\n",
    "        df_sys_l['type'] = art\n",
    "        df_sys_g = pd.concat([df_sys_g, df_sys_l ], ignore_index=True, sort=False)\n",
    "    \n",
    "    return df_sys_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-11 23:50:14,606 : INFO : artifacts: 67\n",
      "2021-01-11 23:50:14,619 : INFO : artifacts: 100\n"
     ]
    }
   ],
   "source": [
    "df_test = loading_artifacts( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ManagerClassroom.java</td>\n",
       "      <td>package smos.storage;\\n\\n\\nimport java.sql.Con...</td>\n",
       "      <td>src</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletUpdateUser.java</td>\n",
       "      <td>package smos.application.userManagement;\\n\\nim...</td>\n",
       "      <td>src</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletComputateStatistics.java</td>\n",
       "      <td>package smos.application.registerManagement;\\n...</td>\n",
       "      <td>src</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletProva.java</td>\n",
       "      <td>package smos.application.teachingManagement;\\n...</td>\n",
       "      <td>src</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletInitialize.java</td>\n",
       "      <td>package smos.application;\\n\\nimport javax.serv...</td>\n",
       "      <td>src</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ids  \\\n",
       "67  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "68  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "69  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "70  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "71  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "\n",
       "                          filenames  \\\n",
       "67            ManagerClassroom.java   \n",
       "68           ServletUpdateUser.java   \n",
       "69  ServletComputateStatistics.java   \n",
       "70                ServletProva.java   \n",
       "71           ServletInitialize.java   \n",
       "\n",
       "                                                 text type  \n",
       "67  package smos.storage;\\n\\n\\nimport java.sql.Con...  src  \n",
       "68  package smos.application.userManagement;\\n\\nim...  src  \n",
       "69  package smos.application.registerManagement;\\n...  src  \n",
       "70  package smos.application.teachingManagement;\\n...  src  \n",
       "71  package smos.application;\\n\\nimport javax.serv...  src  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['type']=='src'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def processing_artifacts( model_prefix, df_sys_all, funct_params ):\n",
    "    df_sys_all = df_sys_all.copy()\n",
    "    for bpe in model_prefix.keys(): #BPE Preprocessing\n",
    "        prep = ConventionalPreprocessing( funct_params( model_prefix[bpe] ) , bpe = True) #Creating the Preprocessing Object\n",
    "        df_sys_all[ bpe ] = prep.bpe_pieces_pipeline( df_sys_all['text'].values ) \n",
    "        \n",
    "    df_sys_all['conv'] = prep.fromdocs_pipeline( df_sys_all['text'].values ) #Conventional Preprocessing\n",
    "    return df_sys_all, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-11 23:51:45,591 : INFO : fromtokens_pipeline: clean punctuation\n",
      "2021-01-11 23:51:45,627 : INFO : fromtokens_pipeline: camel case\n",
      "2021-01-11 23:51:45,630 : INFO : fromtokens_pipeline: lowe case\n",
      "2021-01-11 23:51:45,631 : INFO : fromtokens_pipeline: white space removed\n",
      "2021-01-11 23:51:45,663 : INFO : fromtokens_pipeline: WordPunctTokenizer\n",
      "2021-01-11 23:51:45,922 : INFO : fromtokens_pipeline: Stop words\n",
      "2021-01-11 23:51:46,655 : INFO : fromtokens_pipeline: Stemmings\n",
      "2021-01-11 23:51:46,670 : INFO : fromtokens_pipeline: Removed Special Terns\n",
      "2021-01-11 23:51:46,672 : INFO : fromtokens_pipeline END\n"
     ]
    }
   ],
   "source": [
    "df_test_sys,r_prep = processing_artifacts( model_prefix = model_prefix, \n",
    "                                          df_sys_all = df_test, \n",
    "                                          funct_params = smos_params #itrust_params#ebt_params\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>bpe8k</th>\n",
       "      <th>bpe32k</th>\n",
       "      <th>bpe128k</th>\n",
       "      <th>conv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>SMOS37.txt</td>\n",
       "      <td>Nome: InserisciNota\\nAttori: Amministratore\\nD...</td>\n",
       "      <td>uc</td>\n",
       "      <td>[▁n, ome, :, ▁ins, er, isc, in, ota, \\n, att, ...</td>\n",
       "      <td>[▁n, ome, :, ▁ins, er, isc, in, ota, \\n, att, ...</td>\n",
       "      <td>[▁nome, :, ▁ins, er, isc, in, ota, \\n, att, or...</td>\n",
       "      <td>nome inserisci nota attori amministrator descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>SMOS16.txt</td>\n",
       "      <td>Nome: EliminaClasse\\nAttori: Amministratore\\nD...</td>\n",
       "      <td>uc</td>\n",
       "      <td>[▁n, ome, :, ▁elimin, ac, las, se, \\n, att, or...</td>\n",
       "      <td>[▁n, ome, :, ▁elimin, ac, las, se, \\n, att, or...</td>\n",
       "      <td>[▁nome, :, ▁elimin, ac, lasse, \\n, att, ori, :...</td>\n",
       "      <td>nome elimina class attori amministrator descri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>SMOS24.txt</td>\n",
       "      <td>Nome:VisualizzaDettagliInsegnamento\\nAttori: A...</td>\n",
       "      <td>uc</td>\n",
       "      <td>[▁n, ome, :, vis, ual, iz, z, ad, ett, ag, li,...</td>\n",
       "      <td>[▁n, ome, :, vis, ual, izz, ad, ett, ag, li, i...</td>\n",
       "      <td>[▁nome, :, visual, izz, ad, ett, agli, in, se,...</td>\n",
       "      <td>nome visualizza dettag insegnamento attori amm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>SMOS36.txt</td>\n",
       "      <td>Nome: InserisciGiustifica\\nAttori: Amministrat...</td>\n",
       "      <td>uc</td>\n",
       "      <td>[▁n, ome, :, ▁ins, er, isc, ig, i, ust, ific, ...</td>\n",
       "      <td>[▁n, ome, :, ▁ins, er, isc, igi, ust, ific, a,...</td>\n",
       "      <td>[▁nome, :, ▁ins, er, isc, igi, ust, ific, a, \\...</td>\n",
       "      <td>nome inserisci giustifica attori amministrator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>SMOS26.txt</td>\n",
       "      <td>Nome: EliminaInsegnamento\\nAttori: Amministrat...</td>\n",
       "      <td>uc</td>\n",
       "      <td>[▁n, ome, :, ▁elimin, ain, se, gn, ament, o, \\...</td>\n",
       "      <td>[▁n, ome, :, ▁elimin, ain, se, gn, amento, \\n,...</td>\n",
       "      <td>[▁nome, :, ▁elimin, ain, se, gn, amento, \\n, a...</td>\n",
       "      <td>nome elimina insegnamento attori amministrator...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ids   filenames  \\\n",
       "0  ../dvc-ds4se/se-benchmarking/traceability/data...  SMOS37.txt   \n",
       "1  ../dvc-ds4se/se-benchmarking/traceability/data...  SMOS16.txt   \n",
       "2  ../dvc-ds4se/se-benchmarking/traceability/data...  SMOS24.txt   \n",
       "3  ../dvc-ds4se/se-benchmarking/traceability/data...  SMOS36.txt   \n",
       "4  ../dvc-ds4se/se-benchmarking/traceability/data...  SMOS26.txt   \n",
       "\n",
       "                                                text type  \\\n",
       "0  Nome: InserisciNota\\nAttori: Amministratore\\nD...   uc   \n",
       "1  Nome: EliminaClasse\\nAttori: Amministratore\\nD...   uc   \n",
       "2  Nome:VisualizzaDettagliInsegnamento\\nAttori: A...   uc   \n",
       "3  Nome: InserisciGiustifica\\nAttori: Amministrat...   uc   \n",
       "4  Nome: EliminaInsegnamento\\nAttori: Amministrat...   uc   \n",
       "\n",
       "                                               bpe8k  \\\n",
       "0  [▁n, ome, :, ▁ins, er, isc, in, ota, \\n, att, ...   \n",
       "1  [▁n, ome, :, ▁elimin, ac, las, se, \\n, att, or...   \n",
       "2  [▁n, ome, :, vis, ual, iz, z, ad, ett, ag, li,...   \n",
       "3  [▁n, ome, :, ▁ins, er, isc, ig, i, ust, ific, ...   \n",
       "4  [▁n, ome, :, ▁elimin, ain, se, gn, ament, o, \\...   \n",
       "\n",
       "                                              bpe32k  \\\n",
       "0  [▁n, ome, :, ▁ins, er, isc, in, ota, \\n, att, ...   \n",
       "1  [▁n, ome, :, ▁elimin, ac, las, se, \\n, att, or...   \n",
       "2  [▁n, ome, :, vis, ual, izz, ad, ett, ag, li, i...   \n",
       "3  [▁n, ome, :, ▁ins, er, isc, igi, ust, ific, a,...   \n",
       "4  [▁n, ome, :, ▁elimin, ain, se, gn, amento, \\n,...   \n",
       "\n",
       "                                             bpe128k  \\\n",
       "0  [▁nome, :, ▁ins, er, isc, in, ota, \\n, att, or...   \n",
       "1  [▁nome, :, ▁elimin, ac, lasse, \\n, att, ori, :...   \n",
       "2  [▁nome, :, visual, izz, ad, ett, agli, in, se,...   \n",
       "3  [▁nome, :, ▁ins, er, isc, igi, ust, ific, a, \\...   \n",
       "4  [▁nome, :, ▁elimin, ain, se, gn, amento, \\n, a...   \n",
       "\n",
       "                                                conv  \n",
       "0  nome inserisci nota attori amministrator descr...  \n",
       "1  nome elimina class attori amministrator descri...  \n",
       "2  nome visualizza dettag insegnamento attori amm...  \n",
       "3  nome inserisci giustifica attori amministrator...  \n",
       "4  nome elimina insegnamento attori amministrator...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../dvc-ds4se/se-benchmarking/traceability/datasets/formatted/semeru_format/SMOS_semeru_format/use_cases/SMOS37.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sys['ids'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>bpe8k</th>\n",
       "      <th>bpe32k</th>\n",
       "      <th>bpe128k</th>\n",
       "      <th>conv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ManagerClassroom.java</td>\n",
       "      <td>package smos.storage;\\n\\n\\nimport java.sql.Con...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., st, or, age, ;, \\n\\n\\...</td>\n",
       "      <td>[▁package, ▁sm, os, ., stor, age, ;, \\n\\n\\n, i...</td>\n",
       "      <td>[▁package, ▁sm, os, ., storage, ;, \\n\\n\\n, imp...</td>\n",
       "      <td>packag smos storag import java sql connect imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletUpdateUser.java</td>\n",
       "      <td>package smos.application.userManagement;\\n\\nim...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., us, e...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., us, erm...</td>\n",
       "      <td>packag smos applic user manag import smos envi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletComputateStatistics.java</td>\n",
       "      <td>package smos.application.registerManagement;\\n...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., r...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., reg, ...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., registe...</td>\n",
       "      <td>packag smos applic regist manag import java io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletProva.java</td>\n",
       "      <td>package smos.application.teachingManagement;\\n...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., t...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., te, a...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., teachin...</td>\n",
       "      <td>packag smos applic teach manag import javax se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletInitialize.java</td>\n",
       "      <td>package smos.application;\\n\\nimport javax.serv...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ;, \\...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ;, \\n\\n,...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ;, \\n\\n, i...</td>\n",
       "      <td>packag smos applic import javax servlet servle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletReportTeachings.java</td>\n",
       "      <td>package smos.application.userManagement;\\n\\nim...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., us, e...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., us, erm...</td>\n",
       "      <td>packag smos applic user manag import smos envi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletAssignRole.java</td>\n",
       "      <td>package smos.application.userManagement;\\n\\nim...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., us, e...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., us, erm...</td>\n",
       "      <td>packag smos applic user manag import smos envi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>display.java</td>\n",
       "      <td>\\n\\n/****************** displaytag stylesheet ...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁, \\n\\n, /, *, *, *, *, *, *, *, *, *, *, *, ...</td>\n",
       "      <td>[▁, \\n\\n, /, *, *, *, *, *, *, *, *, *, *, *, ...</td>\n",
       "      <td>[▁, \\n\\n, /, **, **, **, **, **, **, **, **, *...</td>\n",
       "      <td>displaytag stylesheet tabl datat border color ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletInsertReport.java</td>\n",
       "      <td>package smos.application.reportManagement;\\n\\n...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., r...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., re, p...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., report,...</td>\n",
       "      <td>packag smos applic report manag import smos en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>../dvc-ds4se/se-benchmarking/traceability/data...</td>\n",
       "      <td>ServletShowAddressDetails.java</td>\n",
       "      <td>package smos.application.addressManagement;\\n\\...</td>\n",
       "      <td>src</td>\n",
       "      <td>[▁pack, age, ▁sm, os, ., ap, pl, ication, ., a...</td>\n",
       "      <td>[▁package, ▁sm, os, ., appl, ication, ., add, ...</td>\n",
       "      <td>[▁package, ▁sm, os, ., application, ., address...</td>\n",
       "      <td>packag smos applic address manag import smos e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  \\\n",
       "67   ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "68   ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "69   ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "70   ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "71   ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "..                                                 ...   \n",
       "162  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "163  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "164  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "165  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "166  ../dvc-ds4se/se-benchmarking/traceability/data...   \n",
       "\n",
       "                           filenames  \\\n",
       "67             ManagerClassroom.java   \n",
       "68            ServletUpdateUser.java   \n",
       "69   ServletComputateStatistics.java   \n",
       "70                 ServletProva.java   \n",
       "71            ServletInitialize.java   \n",
       "..                               ...   \n",
       "162      ServletReportTeachings.java   \n",
       "163           ServletAssignRole.java   \n",
       "164                     display.java   \n",
       "165         ServletInsertReport.java   \n",
       "166   ServletShowAddressDetails.java   \n",
       "\n",
       "                                                  text type  \\\n",
       "67   package smos.storage;\\n\\n\\nimport java.sql.Con...  src   \n",
       "68   package smos.application.userManagement;\\n\\nim...  src   \n",
       "69   package smos.application.registerManagement;\\n...  src   \n",
       "70   package smos.application.teachingManagement;\\n...  src   \n",
       "71   package smos.application;\\n\\nimport javax.serv...  src   \n",
       "..                                                 ...  ...   \n",
       "162  package smos.application.userManagement;\\n\\nim...  src   \n",
       "163  package smos.application.userManagement;\\n\\nim...  src   \n",
       "164  \\n\\n/****************** displaytag stylesheet ...  src   \n",
       "165  package smos.application.reportManagement;\\n\\n...  src   \n",
       "166  package smos.application.addressManagement;\\n\\...  src   \n",
       "\n",
       "                                                 bpe8k  \\\n",
       "67   [▁pack, age, ▁sm, os, ., st, or, age, ;, \\n\\n\\...   \n",
       "68   [▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...   \n",
       "69   [▁pack, age, ▁sm, os, ., ap, pl, ication, ., r...   \n",
       "70   [▁pack, age, ▁sm, os, ., ap, pl, ication, ., t...   \n",
       "71   [▁pack, age, ▁sm, os, ., ap, pl, ication, ;, \\...   \n",
       "..                                                 ...   \n",
       "162  [▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...   \n",
       "163  [▁pack, age, ▁sm, os, ., ap, pl, ication, ., u...   \n",
       "164  [▁, \\n\\n, /, *, *, *, *, *, *, *, *, *, *, *, ...   \n",
       "165  [▁pack, age, ▁sm, os, ., ap, pl, ication, ., r...   \n",
       "166  [▁pack, age, ▁sm, os, ., ap, pl, ication, ., a...   \n",
       "\n",
       "                                                bpe32k  \\\n",
       "67   [▁package, ▁sm, os, ., stor, age, ;, \\n\\n\\n, i...   \n",
       "68   [▁package, ▁sm, os, ., appl, ication, ., us, e...   \n",
       "69   [▁package, ▁sm, os, ., appl, ication, ., reg, ...   \n",
       "70   [▁package, ▁sm, os, ., appl, ication, ., te, a...   \n",
       "71   [▁package, ▁sm, os, ., appl, ication, ;, \\n\\n,...   \n",
       "..                                                 ...   \n",
       "162  [▁package, ▁sm, os, ., appl, ication, ., us, e...   \n",
       "163  [▁package, ▁sm, os, ., appl, ication, ., us, e...   \n",
       "164  [▁, \\n\\n, /, *, *, *, *, *, *, *, *, *, *, *, ...   \n",
       "165  [▁package, ▁sm, os, ., appl, ication, ., re, p...   \n",
       "166  [▁package, ▁sm, os, ., appl, ication, ., add, ...   \n",
       "\n",
       "                                               bpe128k  \\\n",
       "67   [▁package, ▁sm, os, ., storage, ;, \\n\\n\\n, imp...   \n",
       "68   [▁package, ▁sm, os, ., application, ., us, erm...   \n",
       "69   [▁package, ▁sm, os, ., application, ., registe...   \n",
       "70   [▁package, ▁sm, os, ., application, ., teachin...   \n",
       "71   [▁package, ▁sm, os, ., application, ;, \\n\\n, i...   \n",
       "..                                                 ...   \n",
       "162  [▁package, ▁sm, os, ., application, ., us, erm...   \n",
       "163  [▁package, ▁sm, os, ., application, ., us, erm...   \n",
       "164  [▁, \\n\\n, /, **, **, **, **, **, **, **, **, *...   \n",
       "165  [▁package, ▁sm, os, ., application, ., report,...   \n",
       "166  [▁package, ▁sm, os, ., application, ., address...   \n",
       "\n",
       "                                                  conv  \n",
       "67   packag smos storag import java sql connect imp...  \n",
       "68   packag smos applic user manag import smos envi...  \n",
       "69   packag smos applic regist manag import java io...  \n",
       "70   packag smos applic teach manag import javax se...  \n",
       "71   packag smos applic import javax servlet servle...  \n",
       "..                                                 ...  \n",
       "162  packag smos applic user manag import smos envi...  \n",
       "163  packag smos applic user manag import smos envi...  \n",
       "164  displaytag stylesheet tabl datat border color ...  \n",
       "165  packag smos applic report manag import smos en...  \n",
       "166  packag smos applic address manag import smos e...  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sys[df_test_sys['filenames'].str.contains('.java', regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-11 23:46:32,071 : INFO : Saving in...../dvc-ds4se/se-benchmarking/traceability/testbeds/processed/[itrust-all-corpus-1610408791.737875].csv\n"
     ]
    }
   ],
   "source": [
    "r_prep.SaveCorpus(df_test_sys, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_prep.LoadCorpus(1609221582.171744,language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing from Semeru Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special Case EBT To Create Separate Files [Only one implementation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canonical EBT\n",
    "def ebt_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'ebt',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'EBT_semeru_format/requirements.txt'),\n",
    "            'tc': pathlib.Path( semeru_format + 'EBT_semeru_format/test_cases.txt'),\n",
    "            'src':pathlib.Path( semeru_format + 'EBT_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT'],\n",
    "        'model_prefix':model_prefix,\n",
    "        #'encoding':'ISO-8859-1'\n",
    "        'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ebt_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['dataset']['req']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ebt = pd.read_fwf(params['dataset']['req'],header=None,sep=\"/t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params['dataset']['tc']) as fp:\n",
    "    Lines = fp.readlines()\n",
    "    for line in Lines: \n",
    "        print(line.split(\"\\t\"))\n",
    "        l = line.split(\"\\t\")\n",
    "        p = semeru_format + 'EBT_semeru_format/test_cases/'+l[0]+'.txt'\n",
    "        with open(p, \"w\") as wp: \n",
    "            wp.writelines(l[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filenames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the function\n",
    "#base_dir = os.path.abspath(os.getcwd())\n",
    "test_dir = pathlib.Path('test_data/LibEST_semeru_format/test')\n",
    "#path = os.path.join(base_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all files in a folder\n",
    "for filename in glob.glob(os.path.join(test_dir, '*.txt')):\n",
    "    with open(filename, 'r') as f: # open in readonly mode\n",
    "        dict_filenames[filename] = [f.read()]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(test_dir,filename) for filename in os.listdir(test_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all files in a folder\n",
    "for filename in [os.path.join(test_dir,filename) for filename in os.listdir(test_dir)]:\n",
    "    with open(filename, 'r') as f: # open in readonly mode\n",
    "        dict_filenames[filename] = [f.read()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename('test_data/LibEST_semeru_format/requirements/RQ17.txt').replace('.txt', '-pre.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'[^a-zA-Z\\s]', ' ', \"Ho:;<le_C$%&\\oMe_estTa?@[\\\\is34~\", re.I|re.A).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms(clean_punctuation(\"their corresponding URIs:\\n\\n\\n   +------------------------+-----------------+-------------------+\\n   | Operation              |Operation path   | Details           |\\n   +========================+=================+===================+\\n   | Distribution of CA     | /cacerts        | Section 4.1       |\\n   | Certificates (MUST)    |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Enrollment of          | /simpleenroll   | Section 4.2       |\\n   | Clients (MUST)         |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Re-enrollment of       | /simplereenroll | Section 4.2.2     |\\n   | Clients (MUST)         |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Full CMC (OPTIONAL)    | /fullcmc        | Section 4.3       |\\n   +------------------------+-----------------+-------------------+\\n   | Server-Side Key        | /serverkeygen   | Section 4.4       |\\n   | Generation (OPTIONAL)  |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | CSR Attributes         | /csrattrs       | Section 4.5       |\\n   | (OPTIONAL)             |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n\\n  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms(split_camel_case_token(dict_filenames['test_data/LibEST_semeru_format/requirements/RQ17.txt'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = preprocess_pipeline.basic_pipeline(dict_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Into A File\n",
    "df_pre_processed = pd.DataFrame(pre_process, columns =['filename', 'text']) \n",
    "#/.../benchmarking/traceability/testbeds/nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.getcwd())\n",
    "pre_path = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv'\n",
    "final_path = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-tc].csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processed.to_csv(pre_path, header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(a_dict, path):\n",
    "    a_file = open(path, \"w\")\n",
    "\n",
    "    writer = csv.writer(a_file)\n",
    "    for key, value in a_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Building the corpus vocabulary\n",
    "tokenizer_corpora = text.Tokenizer()\n",
    "tokenizer_corpora.fit_on_texts([doc[1] for doc in pre_process])\n",
    "\n",
    "word2id = tokenizer_corpora.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(id2word,final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging All the Vocabulary\n",
    "vocab_path_tc = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-tc].csv'\n",
    "df_read_vocab_tc = pd.read_csv(vocab_path_tc, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_tc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path_src = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-src].csv'\n",
    "df_read_vocab_src = pd.read_csv(vocab_path_src, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path_req = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-req].csv'\n",
    "df_read_vocab_req = pd.read_csv(vocab_path_req, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_req.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_vocab_set = set(df_read_vocab_tc['text']) | set(df_read_vocab_src['text']) | set(df_read_vocab_req['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(super_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_vocab = pd.DataFrame(list(super_vocab_set))\n",
    "print(df_all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_vocab.to_csv('/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-all].csv', \n",
    "                    header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the corpuses\n",
    "pre_doc_path_tc = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv'\n",
    "pre_doc_path_req = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv'\n",
    "pre_doc_path_src = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-src].csv'\n",
    "\n",
    "#df_read_pre_tc = pd.read_csv(pre_doc_path_tc, header=None, sep=' ') #Need to inclide sep \n",
    "pre_doc_path = [pre_doc_path_tc, pre_doc_path_req, pre_doc_path_src]\n",
    "lis= [list(df_read[1]) for df_read in [pd.read_csv(path, header=None, sep=' ')for path in pre_doc_path]]\n",
    "print(len(lis[0]), len(lis[1]), len(lis[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = functools.reduce(lambda a,b : a+b,lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_pre = pd.DataFrame(lis) \n",
    "df_reduced_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_pre.to_csv('/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Unsupervised Approaches for SE Traceability [approach]\n",
    "\n",
    "> This module is dedicated to evaluate word2vec/doc2vec or any neural unsupervised approaches on traceability datasets. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os\n",
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mgmnt.prep.conv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class VectorizationType(Enum):\n",
    "    word2vec = auto()\n",
    "    doc2vec = auto()\n",
    "    vsm2vec = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizationType.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()\n",
    "    issue2src = auto()\n",
    "    pr2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class DistanceMetric(Enum):\n",
    "    WMD = auto()\n",
    "    COS = auto()\n",
    "    SCM = auto()\n",
    "    EUC = auto()\n",
    "    MAN = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class SimilarityMetric(Enum):\n",
    "    WMD_sim = auto()\n",
    "    COS_sim = auto()\n",
    "    SCM_sim = auto()\n",
    "    EUC_sim = auto()\n",
    "    MAN_sim = auto()\n",
    "    Pearson = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMetric(Enum):\n",
    "    MSI_I = auto() #Minimum shared information Entropy\n",
    "    MSI_X = auto() #Minimum shared information Extropy\n",
    "    MI = auto() #Mutual information\n",
    "    JI = auto() #Joint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftwareArtifacts(Enum):\n",
    "    REQ = auto()\n",
    "    TC = auto()\n",
    "    SRC = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class Preprocessing(Enum):\n",
    "    conv = auto()\n",
    "    bpe = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinkType.req2tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing.bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_trained_model = path_data+'models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "#path_to_trained_model = path_data/'models/wv/bpe128k/[word2vec-Java-Py-Wiki-SK-500-20E-128k[15]-1595189771.501188].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing default params\n",
    "def default_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'sacp',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        #\"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        #\"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": path_data +'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 1 with Libest Conv preprocessing\n",
    "def libest_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.REQ,\n",
    "        \"target_type\": SoftwareArtifacts.TC,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 with Libest BPE preprocessing\n",
    "def libest_params_bpe():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'req', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'tc',\n",
    "        #\"path_mappings\": 'cisco/libest_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = default_params()\n",
    "#parameters = libest_params()\n",
    "#parameters = _params()\n",
    "parameters = sacp_params_bpe()\n",
    "#parameters = libest_params_bpe()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing experiments set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['system_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['names'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['system_path_config']['sep'] #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system = pd.read_csv(\n",
    "            parameters['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = parameters['system_path_config']['sep'] \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "tag = parameters['system_path_config']['names'][1]\n",
    "[doc.split() for doc in df_all_system[df_all_system[tag].notnull()][tag].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_system[tag].values) #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "len(df_all_system[df_all_system[tag].notnull()]) #some files are _init_ thefore are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system[df_all_system[tag].notnull()][tag].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system.loc[df_all_system['type'] == parameters['source_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.loc[df_all_system['type'] == parameters['target_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "print(list(VectorizationType), list(DistanceMetric), list(SimilarityMetric), list(LinkType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicSequenceVectorization():\n",
    "    '''Implementation of the class sequence-vanilla-vectorization other classes can inheritance this one'''\n",
    "    def __init__(self, params):\n",
    "                \n",
    "        self.params = params\n",
    "        self.df_nonground_link = None\n",
    "        self.df_ground_link = None\n",
    "        self.prep = ConventionalPreprocessing(params, bpe = True)\n",
    "        \n",
    "        self.df_all_system = pd.read_csv(\n",
    "            params['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = params['system_path_config']['sep'] \n",
    "        )\n",
    "        \n",
    "        #self.df_source = pd.read_csv(params['source_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        #self.df_target = pd.read_csv(params['target_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        self.df_source = self.df_all_system.loc[self.df_all_system['type'] == params['source_type']][params['system_path_config']['names']]\n",
    "        self.df_target = self.df_all_system.loc[self.df_all_system['type'] == params['target_type']][params['system_path_config']['names']]\n",
    "        \n",
    "        #NA verification\n",
    "        tag = parameters['system_path_config']['names'][1]\n",
    "        self.df_source[tag] = self.df_source[tag].fillna(\"\")\n",
    "        self.df_target[tag] = self.df_target[tag].fillna(\"\")\n",
    "        \n",
    "        if params['system_path_config']['prep'] == Preprocessing.conv: #if conventional preprocessing\n",
    "            self.documents = [doc.split() for doc in self.df_all_system[self.df_all_system[tag].notnull()][tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"conventional preprocessing documents and dictionary\")\n",
    "        elif params['system_path_config']['prep'] == Preprocessing.bpe:\n",
    "            self.documents = [eval(doc) for doc in self.df_all_system[tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"bpe preprocessing documents and dictionary\")\n",
    "            \n",
    "        ####INFO science params\n",
    "        abstracted_vocab = [ set(doc) for doc in self.df_all_system[ 'bpe8k' ].values] #creation of sets\n",
    "        abstracted_vocab = functools.reduce( lambda a,b : a.union(b), abstracted_vocab ) #union of sets\n",
    "        self.vocab = {self.prep.sp_bpe.id_to_piece(id): 0 for id in range(self.prep.sp_bpe.get_piece_size())}\n",
    "        dict_abs_vocab = { elem : 0 for elem in abstracted_vocab - set(self.vocab.keys()) } #Ignored vocab by BPE\n",
    "        self.vocab.update(dict_abs_vocab) #Updating\n",
    "        \n",
    "        \n",
    "        #This can be extended for future metrics <---------------------\n",
    "        #TODO include mutual and join information\n",
    "        self.dict_labels = {\n",
    "            DistanceMetric.COS:[DistanceMetric.COS, SimilarityMetric.COS_sim],\n",
    "            SimilarityMetric.Pearson:[SimilarityMetric.Pearson],\n",
    "            DistanceMetric.EUC:[DistanceMetric.EUC, SimilarityMetric.EUC_sim],\n",
    "            DistanceMetric.WMD:[DistanceMetric.WMD, SimilarityMetric.WMD_sim],\n",
    "            DistanceMetric.SCM:[DistanceMetric.SCM, SimilarityMetric.SCM_sim],\n",
    "            DistanceMetric.MAN:[DistanceMetric.MAN, SimilarityMetric.MAN_sim],\n",
    "            EntropyMetric.MSI_I:[EntropyMetric.MSI_I, EntropyMetric.MSI_X],\n",
    "            EntropyMetric.MI:[EntropyMetric.JI, EntropyMetric.MI]\n",
    "        }\n",
    "\n",
    "        \n",
    "    def ground_truth_processing(self, path_to_ground_truth = '', from_mappings = False):\n",
    "        'Optional class when corpus has ground truth. This function create tuples of links'\n",
    "        \n",
    "        if from_mappings:\n",
    "            df_mapping = pd.read_csv(self.params['path_mappings'], header = 0, sep = ',')\n",
    "            ground_links = list(zip(df_mapping['id_pr'].astype(str), df_mapping['doc_id']))\n",
    "        else:\n",
    "            ground_truth = open(path_to_ground_truth,'r')\n",
    "            #Organizing The Ground Truth under the given format\n",
    "            ground_links = [ [(line.strip().split()[0], elem) for elem in line.strip().split()[1:]] for line in ground_truth]\n",
    "            ground_links = functools.reduce(lambda a,b : a+b,ground_links) #reducing into one list\n",
    "            assert len(ground_links) ==  len(set(ground_links)) #To Verify Redundancies in the file\n",
    "        return ground_links\n",
    "    \n",
    "    def samplingLinks(self, sampling = False, samples = 10, basename = False):\n",
    "        \n",
    "        if basename:\n",
    "            source = [os.path.basename(elem) for elem in self.df_source['ids'].values ] \n",
    "            target = [os.path.basename(elem) for elem in self.df_target['ids'].values ]\n",
    "        else:\n",
    "            source = self.df_source['ids'].values\n",
    "            target = self.df_target['ids'].values\n",
    "\n",
    "        if sampling:\n",
    "            links = sample( list( product( source , target ) ), samples)\n",
    "        else:\n",
    "            links = list( product( source , target ))\n",
    "\n",
    "        return links\n",
    "    \n",
    "    def cos_scipy(self, vector_v, vector_w):\n",
    "        cos =  distance.cosine( vector_v, vector_w )\n",
    "        return [cos, 1.-cos]\n",
    "    \n",
    "    def euclidean_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.euclidean(vector_v,vector_w)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def manhattan_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.cityblock(vector_v,vector_w)\n",
    "        n = len(vector_v)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def pearson_abs_scipy(self, vector_v, vector_w):\n",
    "        '''We are not sure that pearson correlation works well on doc2vec inference vectors'''\n",
    "        #vector_v =  np.asarray(vector_v, dtype=np.float32)\n",
    "        #vector_w =  np.asarray(vector_w, dtype=np.float32)\n",
    "        logging.info(\"pearson_abs_scipy\" + str(vector_v) + \"__\" + str(vector_w))\n",
    "        corr, _ = pearsonr(vector_v, vector_w)\n",
    "        return [abs(corr)] #Absolute value of the correlation\n",
    "    \n",
    "\n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''Metric List Iteration''' \n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "    \n",
    "    def ComputeDistanceArtifacts(self, metric_list, sampling = False , samples = 10, basename = False):\n",
    "        '''Activates Distance and Similarity Computations\n",
    "        @metric_list if [] then Computes All metrics\n",
    "        @sampling is False by the default\n",
    "        @samples is the number of samples (or links) to be generated'''\n",
    "        links_ = self.samplingLinks( sampling, samples, basename )\n",
    "        \n",
    "        docs, metric_labels = self.computeDistanceMetric( metric_list=metric_list, links=links_) #checkpoints\n",
    "        self.df_nonground_link = pd.DataFrame(docs, columns =[self.params['names'][0], self.params['names'][1]]+ metric_labels) #Transforming into a Pandas\n",
    "        logging.info(\"Non-groundtruth links computed\")\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def SaveLinks(self, grtruth=False, sep=' ', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system'] + '-' + str(self.params['vectorizationType']) + '-' + str(self.params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "        \n",
    "        if grtruth:\n",
    "            self.df_ground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        else:\n",
    "            self.df_nonground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def findDistInDF(self, g_tuple, from_mappings=False, semeru_format=False):\n",
    "        '''Return the index values of the matched mappings\n",
    "        .eq is used for Source since it must match the exact code to avoid number substrings\n",
    "        for the target, the substring might works fine'''\n",
    "\n",
    "        if from_mappings:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].eq(g_tuple[0]) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: from_mappings')\n",
    "        elif semeru_format:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].str.contains(g_tuple[0], regex=False) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: semeru_format')\n",
    "        else:\n",
    "            dist = self.df_ground_link[self.df_ground_link[self.params['names'][0]].str.contains( g_tuple[0][:g_tuple[0].find('.')] + '-' ) \n",
    "                     & self.df_ground_link[self.params['names'][1]].str.contains(g_tuple[1][:g_tuple[1].find('.')]) ]\n",
    "            logging.info('findDistInDF: default')\n",
    "        return dist.index.values\n",
    "    \n",
    "        \n",
    "    def MatchWithGroundTruth(self, path_to_ground_truth='', from_mappings=False, semeru_format=False ):\n",
    "        self.df_ground_link = self.df_nonground_link.copy()\n",
    "        self.df_ground_link[self.params['names'][2]] = 0\n",
    "        \n",
    "        matchGT = [ self.findDistInDF( g , from_mappings=from_mappings, semeru_format=semeru_format ) for g in self.ground_truth_processing(path_to_ground_truth,from_mappings)]\n",
    "        matchGT = functools.reduce(lambda a,b : np.concatenate([a,b]), matchGT) #Concatenate indexes\n",
    "        new_column = pd.Series(np.full([len(matchGT)], 1 ), name=self.params['names'][2], index = matchGT)\n",
    "        \n",
    "        self.df_ground_link.update(new_column)\n",
    "        logging.info(\"Groundtruth links computed\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec =  BasicSequenceVectorization(params = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.shape #data final tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for libest\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "general2vec.ground_truth_processing(path_to_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for sacp\n",
    "general2vec.ground_truth_processing(parameters['path_mappings'], from_mappings = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Word2VecSeqVect(BasicSequenceVectorization):       \n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Word2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        #Computes cosine similarities between word embeddings and retrieves the closest \n",
    "        #word embeddings by cosine similarity for a given word embedding.\n",
    "        self.similarity_index = WordEmbeddingSimilarityIndex(self.new_model.wv)\n",
    "        #Build a term similarity matrix and compute the Soft Cosine Measure.\n",
    "        self.similarity_matrix = SparseTermSimilarityMatrix(self.similarity_index, self.dictionary)\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.WMD: self.wmd_gensim,\n",
    "            DistanceMetric.SCM: self.scm_gensim,\n",
    "            EntropyMetric.MSI_I: self.msi,\n",
    "            EntropyMetric.MI: self.mutual_info\n",
    "        }\n",
    "    \n",
    "    def wmd_gensim(self, sentence_a, sentence_b ):\n",
    "        wmd = self.new_model.wv.wmdistance(sentence_a, sentence_b)\n",
    "        return [wmd, self.wmd_similarity(wmd)]\n",
    "    \n",
    "    def wmd_similarity(self, dist):\n",
    "        return 1./( 1.+float( dist ) ) #Associated Similarity\n",
    "    \n",
    "    def scm_gensim(self, sentence_a, sentence_b ):\n",
    "        '''Compute SoftCosine Similarity of Gensim'''\n",
    "        #Convert the sentences into bag-of-words vectors.\n",
    "        sentence_1 = self.dictionary.doc2bow(sentence_a)\n",
    "        sentence_2 = self.dictionary.doc2bow(sentence_b)\n",
    "        \n",
    "        #Return the inner product(s) between real vectors / corpora vec1 and vec2 expressed in a non-orthogonal normalized basis,\n",
    "        #where the dot product between the basis vectors is given by the sparse term similarity matrix.\n",
    "        scm_similarity = self.similarity_matrix.inner_product(sentence_1, sentence_2, normalized=True)\n",
    "        return [1-scm_similarity, scm_similarity]\n",
    "    \n",
    "    def msi(self, sentence_a, sentence_b):\n",
    "        '''@danaderp\n",
    "        Minimum Shared Information'''\n",
    "        token_counts_1 = self.get_cnts(sentence_a, self.vocab)\n",
    "        token_counts_2 = self.get_cnts(sentence_b, self.vocab)\n",
    "        logging.info('token count processed')\n",
    "        #Minimum Shared Tokens\n",
    "        #TODO create an if down to include Joint Entropy by summing token_counts_1 and token_counts_2\n",
    "        token_counts = { token: min(token_counts_1[token],token_counts_2[token]) for token in self.vocab }\n",
    "        \n",
    "        alphabet = list(set(token_counts.keys())) #[ list(set(cnt.keys())) for cnt in token_counts ]\n",
    "        frequencies = self.get_freqs(token_counts) #[ get_freqs(cnt) for cnt in token_counts ]\n",
    "        logging.info('frequencies processed')\n",
    "            \n",
    "        if not frequencies:\n",
    "            #\"List is empty\"\n",
    "            entropies = float('nan')\n",
    "            extropies = float('nan')\n",
    "        else:\n",
    "            scalar_distribution = dit.ScalarDistribution(alphabet, frequencies) #[dit.ScalarDistribution(alphabet[id], frequencies[id]) for id in range( len(token_counts) )]\n",
    "            logging.info('scalar_distribution processed')\n",
    "            \n",
    "            entropies = dit.shannon.entropy( scalar_distribution ) #[ dit.shannon.entropy( dist ) for dist in scalar_distribution ]\n",
    "            logging.info('entropies processed')\n",
    "            \n",
    "            extropies = dit.other.extropy( scalar_distribution )# [ dit.other.extropy( dist ) for dist in scalar_distribution ]\n",
    "            logging.info('extropies processed')\n",
    "        return [entropies,extropies]\n",
    "    \n",
    "    def mutual_info(self, sentence_a, sentence_b):\n",
    "        \"\"\"Mutual information and Joint Information\"\"\"\n",
    "        token_counts_1 = self.get_cnts(sentence_a, self.vocab)\n",
    "        token_counts_2 = self.get_cnts(sentence_b, self.vocab)\n",
    "        logging.info('token count processed')\n",
    "\n",
    "\n",
    "        #TODO verify redundancies in the alphabet\n",
    "        alphabet_source = list(set(token_counts_1.keys()))\n",
    "        logging.info('alphabet_source #'+ str(len(alphabet_source)))\n",
    "        alphabet_target = list(set(token_counts_2.keys()))\n",
    "        logging.info('alphabet_target #'+ str(len(alphabet_target)))\n",
    "        \n",
    "        logging.info('vocab #'+ str(len(self.vocab.keys())))\n",
    "        logging.info('diff #'+ str(set(token_counts_1.keys()) - set(token_counts_2.keys())))\n",
    "        #Computing Self-Information (or Entropy)\n",
    "        scalar_distribution_source = dit.ScalarDistribution(alphabet_source, self.get_freqs( token_counts_1 ) )\n",
    "        entropy_source = dit.shannon.entropy( scalar_distribution_source )\n",
    "        \n",
    "        scalar_distribution_target = dit.ScalarDistribution(alphabet_target, self.get_freqs( token_counts_2 ) )\n",
    "        entropy_target = dit.shannon.entropy( scalar_distribution_target )\n",
    "        \n",
    "        #Computing Joint-information\n",
    "        token_counts = { token: (token_counts_1[token] + token_counts_2[token]) for token in self.vocab }\n",
    "        alphabet = list(set(token_counts.keys()))\n",
    "        logging.info('alphabet #'+ str(len(alphabet)))\n",
    "        frequencies = self.get_freqs(token_counts)\n",
    "        ##WARNING! if a document is empty frequencies might create an issue!\n",
    "        scalar_distribution = dit.ScalarDistribution(alphabet, frequencies)\n",
    "        joint_entropy = dit.shannon.entropy( scalar_distribution )\n",
    "        \n",
    "        mutual_information = entropy_source + entropy_target - joint_entropy\n",
    "        return [joint_entropy, mutual_information]\n",
    "    \n",
    "    #ToDo Mutual information\n",
    "    \n",
    "    def distance(self, metric_list,link):\n",
    "        '''Iterate on the metrics'''\n",
    "        #Computation of sentences can be moved directly to wmd_gensim method if we cannot generalize it for \n",
    "        #the remaining metrics\n",
    "        ids = parameters['system_path_config']['names'][0]\n",
    "        txt = parameters['system_path_config']['names'][1]\n",
    "        \n",
    "        if self.params['system_path_config']['prep'] == Preprocessing.conv: #if conventional preprocessing\n",
    "            sentence_a = self.df_source[self.df_source[ids].str.contains(link[0])][txt].values[0].split()\n",
    "            sentence_b = self.df_target[self.df_target[ids].str.contains(link[1])][txt].values[0].split()\n",
    "        elif self.params['system_path_config']['prep'] == Preprocessing.bpe:\n",
    "            sentence_a = eval(self.df_source[self.df_source[ids].str.contains(link[0])][txt].values[0])\n",
    "            sentence_b = eval(self.df_target[self.df_target[ids].str.contains(link[1])][txt].values[0])\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](sentence_a,sentence_b) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list\n",
    "    \n",
    "    #################################3TODO substitute this block in the future by importing information science module\n",
    "    def get_cnts(self, toks, vocab):\n",
    "        '''@danaderp\n",
    "        Counts tokens within ONE document'''\n",
    "        #logging.info(\"encoding_size:\" len\n",
    "        cnt = Counter(vocab)\n",
    "        for tok in toks:\n",
    "            cnt[tok] += 1\n",
    "        return cnt\n",
    "\n",
    "    def get_freqs(self, dict_token_counts):\n",
    "\n",
    "        num_tokens = sum( dict_token_counts.values() ) #number of subwords inside the document\n",
    "        if num_tokens == 0.0:\n",
    "            frequencies = []\n",
    "            logging.info('---------------> NO SHARED INFORMATION <-------------------------')\n",
    "        else:\n",
    "            frequencies = [ (dict_token_counts[token])/num_tokens for token in dict_token_counts ]\n",
    "        return frequencies\n",
    "    #################################3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LoadLinks(timestamp, params, grtruth=False, sep=' ' ):\n",
    "    '''Returns a pandas from a saved link computation at a give timestamp\n",
    "    @timestamp is the version of the model for a given system'''\n",
    "    \n",
    "    path= params['saving_path'] + '['+ params['system'] + '-' + str(params['vectorizationType']) + '-' + str(params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "    \n",
    "    logging.info(\"Loading computed links from... \"+ path)\n",
    "\n",
    "    return pd.read_csv(path, header=0, index_col=0, sep=sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Word2Vec SequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#tst\n",
    "metric_list = ['a','b']\n",
    "A = [[1,3,4],[4,5],[1,8,9,7]]\n",
    "B = ((1,3,4),(4,5),(1,8,9,7))\n",
    "functools.reduce(lambda a,b : a+b, B)\n",
    "dist_sim_T = [([12,13],['metric1','metric2']),([12,13],['metric1','metric2'])]\n",
    "dist_sim_T\n",
    "separated_merged_list_a = functools.reduce(lambda a,b : a[1]+b[1], dist_sim_T)\n",
    "separated_merged_list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = Word2VecSeqVect( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2vec.new_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_source['ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = parameters['system_path_config']['names'][0]\n",
    "txt = parameters['system_path_config']['names'][1]\n",
    "print(ids,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idss = word2vec.df_source[ids][35] #Selecting an ID\n",
    "idss = word2vec.df_source[ids] == idss #Search for an specific ID\n",
    "list(word2vec.df_source[idss][txt])[0].split() #Retrieving text and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = word2vec.samplingLinks(sampling=True, samples = 2)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(links), word2vec.df_source.shape, word2vec.df_target.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "#word2vec.df_source[word2vec.df_source[ids].str.contains(links[0][0])][txt].values[0].split() #conventioanal\n",
    "eval(word2vec.df_source[word2vec.df_source[ids].str.contains(links[0][0])][txt].values[0]) #BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "word2vec.df_target[word2vec.df_target[ids].str.contains(links[0][1])][txt].values[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Example and Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I]\n",
    "metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional] computeDistanceMetric Testing [WARNING!] Time Consuming!!\n",
    "computeDistanceMetric = word2vec.computeDistanceMetric(links, metric_list = metric_list )\n",
    "computeDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 5, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "#df_mapping = pd.read_csv(parameters['path_mappings'], header = 0, sep = ',')\n",
    "ground_links = word2vec.ground_truth_processing(from_mappings='True')\n",
    "ground_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_x = word2vec.df_nonground_link\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_links[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_x[(df_x[\"Source\"].eq(ground_links[0][0]) ) & (df_x[\"Target\"].str.contains(ground_links[0][1], regex=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_gt( tuple_g ):\n",
    "    dist = df_x.loc[(df_x[\"Source\"].str.eq(tuple_g[0]) ) & \n",
    "                 (df_x[\"Target\"].str.contains(tuple_g[1], regex=False))]\n",
    "    return dist.index.values\n",
    "#dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchGT = [ word2vec.findDistInDF( g , from_mappings=True ) for g in word2vec.ground_truth_processing(from_mappings=True)]\n",
    "matchGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchGT = functools.reduce(lambda a,b : np.concatenate([a,b]), matchGT) #Concatenate indexes\n",
    "matchGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = pd.Series(np.full([len(matchGT)], 1 ), name=word2vec.params['names'][2], index = matchGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the mappings are not found in the non-ling list because the mappings have all the ground truth of the issues\n",
    "#it might include files not take into account in the non-links part\n",
    "matchGT_ = [ (g,word2vec.findDistInDF( g , from_mappings=True )) for g in word2vec.ground_truth_processing(from_mappings=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchGT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matchGT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matchGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = LoadLinks(timestamp=1596416340.728148, params=parameters)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z = word2vec.df_ground_link\n",
    "df_z[~df_z.isin([np.nan, np.inf, -np.inf]).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "df_y = word2vec.df_ground_link.copy()\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "df_y.update(new_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link['Linked?'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link['Linked?'] == 1].shape #Positive Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links[141] # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting Source\n",
    "ground_links[141][0][:ground_links[141][0].find('.')] + '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting Target\n",
    "ground_links[141][1][:ground_links[141][1].find('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = LoadLinks(timestamp=1596426181.318831, params=parameters,grtruth = True)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Traceability with Artifacts Representation \n",
    "We are employing two techniques for analyzing software artifacts without groundtruth:\n",
    "- Prototypes and Criticisms for Paragraph Vectors \n",
    "- Information Theory for Software Traceability (Shared Information and Mutual Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (word2vec)\n",
    "Classification/evaluation metrics for highly imbalanced data [(see Forum)](https://stats.stackexchange.com/questions/222558/classification-evaluation-metrics-for-highly-imbalanced-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorEvaluation():\n",
    "    '''Approaches Common Evaluations and Interpretations (statistical analysis)'''\n",
    "    def __init__(self, sequenceVectorization):\n",
    "        self.seqVect = sequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SupervisedVectorEvaluation(VectorEvaluation):\n",
    "    def __init__(self, sequenceVectorization, sim_list):\n",
    "        super().__init__(sequenceVectorization)\n",
    "        self.sim_list = sim_list\n",
    "        \n",
    "        self.df_filtered = sequenceVectorization.df_ground_link \n",
    "        self.df_filtered = self.df_filtered[~self.df_filtered.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "        \n",
    "        #CreateFilters Here\n",
    "        \n",
    "        self.y_test = self.df_filtered['Linked?'].values\n",
    "        self.y_score = [self.df_filtered[sim].values for sim in sim_list]\n",
    "        self.title = str(sequenceVectorization.params['vectorizationType'])\n",
    "        pass\n",
    "    \n",
    "    def Compute_precision_recall_gain(self):\n",
    "        '''One might choose PRG if there is little interest in identifying false negatives '''\n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            prg_curve = prg.create_prg_curve(self.y_test, self.y_score[count])\n",
    "            auprg = prg.calc_auprg(prg_curve)\n",
    "            prg.plot_prg(prg_curve)\n",
    "            logging.info('auprg:  %.3f' %  auprg)\n",
    "            logging.info(\"compute_precision_recall_gain Complete: \"+str(sim))\n",
    "        pass\n",
    "    \n",
    "    def Compute_avg_precision(self):\n",
    "        '''Generated precision-recall curve'''\n",
    "        \n",
    "        # calculate the no skill line as the proportion of the positive class\n",
    "        no_skill = len(self.y_test[self.y_test==1]) / len(self.y_test)\n",
    "        \n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill') #reference curve\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, self.y_score[count]) #compute precision-recall curve\n",
    "            plt.plot(recall, precision, marker='.', label = str(sim)) #plot model curve\n",
    "            plt.title(self.label[count])\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.legend() #show the legend\n",
    "            plt.show() #show the plot\n",
    "\n",
    "            average_precision = average_precision_score(self.y_test, self.y_score[count])\n",
    "            auc_score = auc(recall, precision)\n",
    "            logging.info('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            logging.info('Precision-Recall AUC: %.3f' % auc_score)\n",
    "        pass\n",
    "    \n",
    "    def Compute_avg_precision_same_plot(self):\n",
    "        '''Generated precision-recall curve'''\n",
    "\n",
    "        # calculate the no skill line as the proportion of the positive class\n",
    "        no_skill = len(self.y_test[self.y_test==1]) / len(self.y_test)\n",
    "        plt.plot([0, 1], [no_skill, no_skill], linewidth=0.5, linestyle='--', label='No Skill [{0:0.2f}]'.format(no_skill)) #reference curve\n",
    "        \n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, self.y_score[count]) #compute precision-recall curve\n",
    "            average_precision = average_precision_score(self.y_test, self.y_score[count])\n",
    "            auc_score = auc(recall, precision)\n",
    "            logging.info('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            logging.info('Precision-Recall AUC: %.2f' % auc_score)\n",
    "            \n",
    "            #plt.plot(recall, precision, linewidth=0.4, marker='.', label = str(sim)) #plot model curve\n",
    "            plt.plot(recall, precision, linewidth=1, label = str(sim)+  ' [auc:{0:0.2f}]'.format(auc_score)) #plot model curve\n",
    "            pass\n",
    "        \n",
    "        plt.title(self.title)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend(fontsize=9) #show the legend\n",
    "        plt.show() #show the plot\n",
    "        pass\n",
    "    \n",
    "    def Compute_roc_curve(self):\n",
    "\n",
    "        plt.plot([0, 1], [0, 1],  linewidth=0.5, linestyle='--', label='No Skill') #reference curve\n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, self.y_score[count]) #compute roc curve\n",
    "            roc_auc = roc_auc_score(self.y_test, self.y_score[count])\n",
    "            logging.info('ROC AUC %.2f' % roc_auc)\n",
    "            \n",
    "            plt.plot(fpr, tpr,  linewidth=1, label = str(sim)+  ' [auc:{0:0.2f}]'.format(roc_auc)) #plot model curve\n",
    "            pass\n",
    "        plt.title(self.title)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(fontsize=9) #show the legend\n",
    "        plt.show() #show the plot\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = [SimilarityMetric.SCM_sim, SimilarityMetric.WMD_sim]\n",
    "supevisedEval = SupervisedVectorEvaluation(word2vec, sim_list = similarities ) #<---- Parameter \n",
    "#supevisedEval = SupervisedVectorEvaluation(word2vec, similarity=SimilarityMetric.WMD_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO Move the confusion matrix to SupervisedVectorEvaluation\n",
    "y_score_threshold = [0 if elem<=0.8 else 1 for elem in supevisedEval.y_score] #Hardcoded 0.7 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO a Variation threshold analysis\n",
    "tn, fp, fn, tp = confusion_matrix(supevisedEval.y_test, y_score_threshold).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Racall-Gain\n",
    "Based on the library here: [link](https://github.com/meeliskull/prg/tree/master/Python_package). \n",
    "The area under traditional PR curves can easily favour models with lower expected F1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection [(Flach & Kull, 2015)](http://people.cs.bris.ac.uk/~flach//PRGcurves/).\n",
    "One might choose PRG if there is little interest in identifying false negatives [(from Blog)](https://medium.com/@alexabate/i-did-something-boring-so-you-dont-have-to-9140ca46c84d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average precision score\n",
    "Precision is a metric that quantifies the number of correct positive predictions made.\n",
    "\n",
    "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_avg_precision_same_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ROC Curve\n",
    "An ROC curve (or receiver operating characteristic curve) is a plot that summarizes the performance of a binary classification model on the positive class [(see Blog)](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/).\n",
    "\n",
    "Use ROC when both classes detection is equally important  When we want to give equal weight to both classes prediction ability we should look at the ROC curve [link](https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_metrics = supevisedEval.df_filtered #word2vec.df_ground_link\n",
    "filter_metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(filter_metrics, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_metrics[[SimilarityMetric.WMD_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_metrics[DistanceMetric.WMD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model precision-recall curve\n",
    "sim = np.array(filter_metrics[SimilarityMetric.SCM_sim]) #SimilarityMetric.SCM_sim #SimilarityMetric.WMD_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(column=[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = filter_metrics[[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM]].std()\n",
    "print(errors)\n",
    "filter_metrics[[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[SimilarityMetric.WMD_sim].plot.kde()\n",
    "filter_metrics[SimilarityMetric.WMD_sim].plot.hist(density=True) # Histogram will now be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[SimilarityMetric.SCM_sim].plot.kde()\n",
    "filter_metrics[SimilarityMetric.SCM_sim].plot.hist(density=True) # Histogram will now be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[DistanceMetric.WMD].plot.kde()\n",
    "filter_metrics[DistanceMetric.WMD].plot.hist(density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[DistanceMetric.SCM].plot.kde()\n",
    "filter_metrics[DistanceMetric.SCM].plot.hist(density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=SimilarityMetric.WMD_sim ,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=SimilarityMetric.SCM_sim ,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=DistanceMetric.WMD,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=DistanceMetric.SCM,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_metrics.boxplot(by='Linked?',column=[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM],figsize=[7, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_01 = filter_metrics.copy()\n",
    "filter_metrics_01.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_01[EntropyMetric.MSI_I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman_corr(filter_metrics_01, columns = [EntropyMetric.MSI_I,SimilarityMetric.SCM_sim] ):\n",
    "    df_correlation = filter_metrics_01.copy() \n",
    "    correlation = df_correlation[columns].corr(method='spearman')\n",
    "    #correlation = df_correlation.corr(method='spearman')\n",
    "    return correlation[columns[0]].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Shared Entropy and Word Distance\n",
    "x1 = filter_metrics_01.plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.WMD_sim, \n",
    "    c='DarkBlue', \n",
    "    s=1,\n",
    "    title = 'SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = filter_metrics_01.plot.scatter(\n",
    "    x=EntropyMetric.MSI_X,\n",
    "    y=SimilarityMetric.WMD_sim, \n",
    "    c='DarkBlue', \n",
    "    s=1,\n",
    "    title = 'SCM-Extropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_01,[EntropyMetric.MSI_X,SimilarityMetric.SCM_sim] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_linked = filter_metrics_01[filter_metrics_01['Linked?'] == 1].copy()\n",
    "filter_metrics_nonlinked = filter_metrics_01[filter_metrics_01['Linked?'] == 0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.SCM_sim, \n",
    "    c='Red',\n",
    "    s=1,\n",
    "    title = 'Liked SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_linked)\n",
    ")\n",
    "#x2.text(0,0,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_ = filter_metrics_nonlinked.plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.SCM_sim, \n",
    "    c='DarkBlue',\n",
    "    s=1,\n",
    "    title = 'non-Linked SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_nonlinked)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information levels vs semantics\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separated by ground truth Links!\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separated by ground truth NonLinked!\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 0].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [6, 5],\n",
    "    title = 'Information-Semantic Interactions SCM non-Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax7 = filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_X,\n",
    "    y = EntropyMetric.MSI_I,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM',\n",
    "    colormap = 'viridis',\n",
    "    s=1\n",
    ")\n",
    "ax7.set_xlabel(\"Minimum Shared Extropy\")\n",
    "ax7.set_ylabel(\"Minimum Shared Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 0].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD non-Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reproduce the same empirical evaluation like here: [link](https://arxiv.org/pdf/1507.07998.pdf). Pay attention to:\n",
    "- Accuracy vs. Dimensionality (we can replace accuracy for false positive rate or true positive rate)\n",
    "- Visualize paragraph vectors using t-sne\n",
    "- Computing Cosine Distance and Similarity. More about similarity [link](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-PVDBOW-500-20E-1592609630.689167].model',\n",
    "#\"path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-Wiki-PVDBOW-500-20E[15]-1592941134.367976].model',\n",
    "path_to_trained_model = 'test_data/models/[doc2vec-Py-Java-PVDBOW-500-20E-8k-1594572857.17191].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.doc2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        \"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        \"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": 'test_data/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_params = doc2vec_params()\n",
    "doc2vec_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Doc2VecSeqVect(BasicSequenceVectorization):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Doc2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        self.df_inferred_src = None\n",
    "        self.df_inferred_trg = None\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.EUC: self.euclidean_scipy,\n",
    "            DistanceMetric.MAN: self.manhattan_scipy\n",
    "        }\n",
    "    \n",
    "    def distance(self, metric_list, link):\n",
    "        '''Iterate on the metrics'''\n",
    "        _inferredSource = list(self.df_inferred_src[self.df_inferred_src['ids'].str.contains(link[0])]['inf-doc2vec'])\n",
    "        w_inferredTarget = list(self.df_inferred_trg[self.df_inferred_trg['ids'].str.contains(link[1])]['inf-doc2vec'])\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](_inferredSource,w_inferredTarget) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list\n",
    "    \n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''It is computed the cosine similarity'''\n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "\n",
    "    \n",
    "    def InferDoc2Vec(self, steps=200):\n",
    "        '''Activate Inference on Target and Source Corpus'''\n",
    "        self.df_inferred_src = self.df_source.copy()\n",
    "        self.df_inferred_trg = self.df_target.copy()\n",
    "        \n",
    "        self.df_inferred_src['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_src['text'].values]\n",
    "        self.df_inferred_trg['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_trg['text'].values]\n",
    "        \n",
    "        logging.info(\"Infer Doc2Vec on Source and Target Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Doc2Vec SequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecSeqVect(params = doc2vec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1]Apply Doc2Vec Inference\n",
    "doc2vec.InferDoc2Vec(steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.df_inferred_src.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inferDoc2Vec_trg = inferDoc2Vec(df_target)\n",
    "#test_inferDoc2Vec_trg.head()\n",
    "doc2vec.df_inferred_trg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(doc2vec.df_inferred_trg['inf-doc2vec'][0], doc2vec.df_inferred_trg['inf-doc2vec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_l = [DistanceMetric.EUC,DistanceMetric.COS,DistanceMetric.MAN]# , SimilarityMetric.Pearson]\n",
    "doc2vec.ComputeDistanceArtifacts( sampling=False, samples = 50, metric_list = metric_l )\n",
    "doc2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "doc2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks_doc2vec = LoadLinks(timestamp=1594653325.258415, params=doc2vec_params)\n",
    "df_nonglinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "doc2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "doc2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks_doc2vec = LoadLinks(timestamp=1594653350.19946, params=doc2vec_params, grtruth = True)\n",
    "df_glinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.EUC_sim)\n",
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.COS_sim)\n",
    "supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.MAN_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_avg_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_doc2vec = doc2vec.df_ground_link\n",
    "filter_doc2vec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[[SimilarityMetric.EUC_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[DistanceMetric.EUC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate distance from similarity analysis here\n",
    "errors = filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].std()\n",
    "print(errors)\n",
    "filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=SimilarityMetric.EUC_sim,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=DistanceMetric.EUC,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the distance from the similarity plot\n",
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Doc2vec and Word2vec\n",
    "Please check this post for futher detatils [link](https://stats.stackexchange.com/questions/217614/intepreting-doc2vec-cosine-similarity-between-doc-vectors-and-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Unsupervised Approaches for SE Traceability for SacP doc2Vec [approach]\n",
    "\n",
    "> This module is dedicated to evaluate word2vec/doc2vec or any neural unsupervised approaches on traceability datasets. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os\n",
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mgmnt.prep.conv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class VectorizationType(Enum):\n",
    "    word2vec = auto()\n",
    "    doc2vec = auto()\n",
    "    vsm2vec = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizationType.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()\n",
    "    issue2src = auto()\n",
    "    pr2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class DistanceMetric(Enum):\n",
    "    WMD = auto()\n",
    "    COS = auto()\n",
    "    SCM = auto()\n",
    "    EUC = auto()\n",
    "    MAN = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class SimilarityMetric(Enum):\n",
    "    WMD_sim = auto()\n",
    "    COS_sim = auto()\n",
    "    SCM_sim = auto()\n",
    "    EUC_sim = auto()\n",
    "    MAN_sim = auto()\n",
    "    Pearson = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMetric(Enum):\n",
    "    MSI_I = auto() #Minimum shared information Entropy\n",
    "    MSI_X = auto() #Minimum shared information Extropy\n",
    "    MI = auto() #Mutual information\n",
    "    JI = auto() #Joint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftwareArtifacts(Enum):\n",
    "    REQ = auto()\n",
    "    TC = auto()\n",
    "    SRC = auto()\n",
    "    PY = auto()\n",
    "    PR = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class Preprocessing(Enum):\n",
    "    conv = auto()\n",
    "    bpe = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinkType.req2tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing.bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_trained_model = path_data+'models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": SoftwareArtifacts.PR,\n",
    "        \"target_type\": SoftwareArtifacts.PY,\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR,\n",
    "        \"target_type\": SoftwareArtifacts.PY,\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = sacp_params_bpe()\n",
    "#parameters = sacp_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing experiments set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['system_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['names'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['system_path_config']['sep'] #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system = pd.read_csv(\n",
    "            parameters['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = parameters['system_path_config']['sep'] \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "tag = parameters['system_path_config']['names'][1]\n",
    "[doc.split() for doc in df_all_system[df_all_system[tag].notnull()][tag].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_system[tag].values) #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "len(df_all_system[df_all_system[tag].notnull()]) #some files are _init_ thefore are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system[df_all_system[tag].notnull()][tag].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system.loc[df_all_system['type'] == parameters['source_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.loc[df_all_system['type'] == parameters['target_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "print(list(VectorizationType), list(DistanceMetric), list(SimilarityMetric), list(LinkType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicSequenceVectorization():\n",
    "    '''Implementation of the class sequence-vanilla-vectorization other classes can inheritance this one'''\n",
    "    def __init__(self, params):\n",
    "                \n",
    "        self.params = params\n",
    "        self.df_nonground_link = None\n",
    "        self.df_ground_link = None\n",
    "        self.prep = ConventionalPreprocessing(params, bpe = True)\n",
    "        \n",
    "        self.df_all_system = pd.read_csv(\n",
    "            params['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = params['system_path_config']['sep'] \n",
    "        )\n",
    "        \n",
    "        #self.df_source = pd.read_csv(params['source_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        #self.df_target = pd.read_csv(params['target_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        self.df_source = self.df_all_system.loc[self.df_all_system['type'] == params['source_type']][params['system_path_config']['names']]\n",
    "        self.df_target = self.df_all_system.loc[self.df_all_system['type'] == params['target_type']][params['system_path_config']['names']]\n",
    "        \n",
    "        #NA verification\n",
    "        tag = parameters['system_path_config']['names'][1]\n",
    "        self.df_source[tag] = self.df_source[tag].fillna(\"\")\n",
    "        self.df_target[tag] = self.df_target[tag].fillna(\"\")\n",
    "        \n",
    "        if params['system_path_config']['prep'] == Preprocessing.conv: #if conventional preprocessing\n",
    "            self.documents = [doc.split() for doc in self.df_all_system[self.df_all_system[tag].notnull()][tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"conventional preprocessing documents and dictionary\")\n",
    "        elif params['system_path_config']['prep'] == Preprocessing.bpe:\n",
    "            self.documents = [eval(doc) for doc in self.df_all_system[tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"bpe preprocessing documents and dictionary\")\n",
    "            \n",
    "        ####INFO science params\n",
    "        abstracted_vocab = [ set(doc) for doc in self.df_all_system[ 'bpe8k' ].values] #creation of sets\n",
    "        abstracted_vocab = functools.reduce( lambda a,b : a.union(b), abstracted_vocab ) #union of sets\n",
    "        self.vocab = {self.prep.sp_bpe.id_to_piece(id): 0 for id in range(self.prep.sp_bpe.get_piece_size())}\n",
    "        dict_abs_vocab = { elem : 0 for elem in abstracted_vocab - set(self.vocab.keys()) } #Ignored vocab by BPE\n",
    "        self.vocab.update(dict_abs_vocab) #Updating\n",
    "        \n",
    "        \n",
    "        #This can be extended for future metrics <---------------------\n",
    "        #TODO include mutual and join information\n",
    "        self.dict_labels = {\n",
    "            DistanceMetric.COS:[DistanceMetric.COS, SimilarityMetric.COS_sim],\n",
    "            SimilarityMetric.Pearson:[SimilarityMetric.Pearson],\n",
    "            DistanceMetric.EUC:[DistanceMetric.EUC, SimilarityMetric.EUC_sim],\n",
    "            DistanceMetric.WMD:[DistanceMetric.WMD, SimilarityMetric.WMD_sim],\n",
    "            DistanceMetric.SCM:[DistanceMetric.SCM, SimilarityMetric.SCM_sim],\n",
    "            DistanceMetric.MAN:[DistanceMetric.MAN, SimilarityMetric.MAN_sim],\n",
    "            EntropyMetric.MSI_I:[EntropyMetric.MSI_I, EntropyMetric.MSI_X],\n",
    "            EntropyMetric.MI:[EntropyMetric.JI, EntropyMetric.MI]\n",
    "        }\n",
    "\n",
    "        \n",
    "    def ground_truth_processing(self, path_to_ground_truth = '', from_mappings = False):\n",
    "        'Optional class when corpus has ground truth. This function create tuples of links'\n",
    "        \n",
    "        if from_mappings:\n",
    "            df_mapping = pd.read_csv(self.params['path_mappings'], header = 0, sep = ',')\n",
    "            ground_links = list(zip(df_mapping['id_pr'].astype(str), df_mapping['doc_id']))\n",
    "        else:\n",
    "            ground_truth = open(path_to_ground_truth,'r')\n",
    "            #Organizing The Ground Truth under the given format\n",
    "            ground_links = [ [(line.strip().split()[0], elem) for elem in line.strip().split()[1:]] for line in ground_truth]\n",
    "            ground_links = functools.reduce(lambda a,b : a+b,ground_links) #reducing into one list\n",
    "            assert len(ground_links) ==  len(set(ground_links)) #To Verify Redundancies in the file\n",
    "        return ground_links\n",
    "    \n",
    "    def samplingLinks(self, sampling = False, samples = 10, basename = False):\n",
    "        \n",
    "        if basename:\n",
    "            source = [os.path.basename(elem) for elem in self.df_source['ids'].values ] \n",
    "            target = [os.path.basename(elem) for elem in self.df_target['ids'].values ]\n",
    "        else:\n",
    "            source = self.df_source['ids'].values\n",
    "            target = self.df_target['ids'].values\n",
    "\n",
    "        if sampling:\n",
    "            links = sample( list( product( source , target ) ), samples)\n",
    "        else:\n",
    "            links = list( product( source , target ))\n",
    "\n",
    "        return links\n",
    "    \n",
    "    def cos_scipy(self, vector_v, vector_w):\n",
    "        cos =  distance.cosine( vector_v, vector_w )\n",
    "        return [cos, 1.-cos]\n",
    "    \n",
    "    def euclidean_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.euclidean(vector_v,vector_w)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def manhattan_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.cityblock(vector_v,vector_w)\n",
    "        n = len(vector_v)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def pearson_abs_scipy(self, vector_v, vector_w):\n",
    "        '''We are not sure that pearson correlation works well on doc2vec inference vectors'''\n",
    "        #vector_v =  np.asarray(vector_v, dtype=np.float32)\n",
    "        #vector_w =  np.asarray(vector_w, dtype=np.float32)\n",
    "        logging.info(\"pearson_abs_scipy\" + str(vector_v) + \"__\" + str(vector_w))\n",
    "        corr, _ = pearsonr(vector_v, vector_w)\n",
    "        return [abs(corr)] #Absolute value of the correlation\n",
    "    \n",
    "\n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''Metric List Iteration''' \n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "    \n",
    "    def ComputeDistanceArtifacts(self, metric_list, sampling = False , samples = 10, basename = False):\n",
    "        '''Activates Distance and Similarity Computations\n",
    "        @metric_list if [] then Computes All metrics\n",
    "        @sampling is False by the default\n",
    "        @samples is the number of samples (or links) to be generated'''\n",
    "        links_ = self.samplingLinks( sampling, samples, basename )\n",
    "        \n",
    "        docs, metric_labels = self.computeDistanceMetric( metric_list=metric_list, links=links_) #checkpoints\n",
    "        self.df_nonground_link = pd.DataFrame(docs, columns =[self.params['names'][0], self.params['names'][1]]+ metric_labels) #Transforming into a Pandas\n",
    "        logging.info(\"Non-groundtruth links computed\")\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def SaveLinks(self, grtruth=False, sep=' ', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system'] + '-' + str(self.params['vectorizationType']) + '-' + str(self.params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "        \n",
    "        if grtruth:\n",
    "            self.df_ground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        else:\n",
    "            self.df_nonground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def findDistInDF(self, g_tuple, from_mappings=False, semeru_format=False):\n",
    "        '''Return the index values of the matched mappings\n",
    "        .eq is used for Source since it must match the exact code to avoid number substrings\n",
    "        for the target, the substring might works fine'''\n",
    "\n",
    "        if from_mappings:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].eq(g_tuple[0]) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: from_mappings')\n",
    "        elif semeru_format:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].str.contains(g_tuple[0], regex=False) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: semeru_format')\n",
    "        else:\n",
    "            dist = self.df_ground_link[self.df_ground_link[self.params['names'][0]].str.contains( g_tuple[0][:g_tuple[0].find('.')] + '-' ) \n",
    "                     & self.df_ground_link[self.params['names'][1]].str.contains(g_tuple[1][:g_tuple[1].find('.')]) ]\n",
    "            logging.info('findDistInDF: default')\n",
    "        return dist.index.values\n",
    "    \n",
    "        \n",
    "    def MatchWithGroundTruth(self, path_to_ground_truth='', from_mappings=False, semeru_format=False ):\n",
    "        self.df_ground_link = self.df_nonground_link.copy()\n",
    "        self.df_ground_link[self.params['names'][2]] = 0\n",
    "        \n",
    "        matchGT = [ self.findDistInDF( g , from_mappings=from_mappings, semeru_format=semeru_format ) for g in self.ground_truth_processing(path_to_ground_truth,from_mappings)]\n",
    "        matchGT = functools.reduce(lambda a,b : np.concatenate([a,b]), matchGT) #Concatenate indexes\n",
    "        new_column = pd.Series(np.full([len(matchGT)], 1 ), name=self.params['names'][2], index = matchGT)\n",
    "        \n",
    "        self.df_ground_link.update(new_column)\n",
    "        logging.info(\"Groundtruth links computed\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec =  BasicSequenceVectorization(params = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.shape #data final tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for libest\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "general2vec.ground_truth_processing(path_to_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for sacp\n",
    "general2vec.ground_truth_processing(parameters['path_mappings'], from_mappings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import dit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reproduce the same empirical evaluation like here: [link](https://arxiv.org/pdf/1507.07998.pdf). Pay attention to:\n",
    "- Accuracy vs. Dimensionality (we can replace accuracy for false positive rate or true positive rate)\n",
    "- Visualize paragraph vectors using t-sne\n",
    "- Computing Cosine Distance and Similarity. More about similarity [link](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-PVDBOW-500-20E-1592609630.689167].model',\n",
    "#\"path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-Wiki-PVDBOW-500-20E[15]-1592941134.367976].model',\n",
    "path_to_trained_model = 'test_data/models/[doc2vec-Py-Java-PVDBOW-500-20E-8k-1594572857.17191].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.doc2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        \"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        \"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": 'test_data/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_params = doc2vec_params()\n",
    "doc2vec_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Doc2VecSeqVect(BasicSequenceVectorization):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Doc2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        self.df_inferred_src = None\n",
    "        self.df_inferred_trg = None\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.EUC: self.euclidean_scipy,\n",
    "            DistanceMetric.MAN: self.manhattan_scipy\n",
    "        }\n",
    "    \n",
    "    def distance(self, metric_list, link):\n",
    "        '''Iterate on the metrics'''\n",
    "        ν_inferredSource = list(self.df_inferred_src[self.df_inferred_src['ids'].str.contains(link[0])]['inf-doc2vec'])\n",
    "        w_inferredTarget = list(self.df_inferred_trg[self.df_inferred_trg['ids'].str.contains(link[1])]['inf-doc2vec'])\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](ν_inferredSource,w_inferredTarget) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list\n",
    "    \n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''It is computed the cosine similarity'''\n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "\n",
    "    \n",
    "    def InferDoc2Vec(self, steps=200):\n",
    "        '''Activate Inference on Target and Source Corpus'''\n",
    "        self.df_inferred_src = self.df_source.copy()\n",
    "        self.df_inferred_trg = self.df_target.copy()\n",
    "        \n",
    "        self.df_inferred_src['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_src['text'].values]\n",
    "        self.df_inferred_trg['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_trg['text'].values]\n",
    "        \n",
    "        logging.info(\"Infer Doc2Vec on Source and Target Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Doc2Vec SequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecSeqVect(params = doc2vec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1]Apply Doc2Vec Inference\n",
    "doc2vec.InferDoc2Vec(steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.df_inferred_src.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inferDoc2Vec_trg = inferDoc2Vec(df_target)\n",
    "#test_inferDoc2Vec_trg.head()\n",
    "doc2vec.df_inferred_trg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(doc2vec.df_inferred_trg['inf-doc2vec'][0], doc2vec.df_inferred_trg['inf-doc2vec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_l = [DistanceMetric.EUC,DistanceMetric.COS,DistanceMetric.MAN]# , SimilarityMetric.Pearson]\n",
    "doc2vec.ComputeDistanceArtifacts( sampling=False, samples = 50, metric_list = metric_l )\n",
    "doc2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "doc2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks_doc2vec = LoadLinks(timestamp=1594653325.258415, params=doc2vec_params)\n",
    "df_nonglinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "doc2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "doc2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks_doc2vec = LoadLinks(timestamp=1594653350.19946, params=doc2vec_params, grtruth = True)\n",
    "df_glinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.EUC_sim)\n",
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.COS_sim)\n",
    "supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.MAN_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_avg_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_doc2vec = doc2vec.df_ground_link\n",
    "filter_doc2vec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[[SimilarityMetric.EUC_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[DistanceMetric.EUC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate distance from similarity analysis here\n",
    "errors = filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].std()\n",
    "print(errors)\n",
    "filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=SimilarityMetric.EUC_sim,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=DistanceMetric.EUC,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the distance from the similarity plot\n",
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Doc2vec and Word2vec\n",
    "Please check this post for futher detatils [link](https://stats.stackexchange.com/questions/217614/intepreting-doc2vec-cosine-similarity-between-doc-vectors-and-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS4SE project",
   "language": "python",
   "name": "ds4se"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

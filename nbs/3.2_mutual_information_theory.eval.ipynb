{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Mutual Information Theory for SE Traceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from prg import prg\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class VectorizationType(Enum):\n",
    "    word2vec = auto()\n",
    "    doc2vec = auto()\n",
    "    vsm2vec = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class DistanceMetric(Enum):\n",
    "    WMD = auto()\n",
    "    COS = auto()\n",
    "    SCM = auto()\n",
    "    EUC = auto()\n",
    "    MAN = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class SimilarityMetric(Enum):\n",
    "    WMD_sim = auto()\n",
    "    COS_sim = auto()\n",
    "    SCM_sim = auto()\n",
    "    EUC_sim = auto()\n",
    "    MAN_sim = auto()\n",
    "    Pearson = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": 'test_data/models/word2vec_libest.model',\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        \"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        \"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": 'test_data/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = default_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VectorizationType.word2vec: 1>, <VectorizationType.doc2vec: 2>, <VectorizationType.vsm2vec: 3>] [<DistanceMetric.WMD: 1>, <DistanceMetric.COS: 2>, <DistanceMetric.SCM: 3>, <DistanceMetric.EUC: 4>, <DistanceMetric.MAN: 5>] [<SimilarityMetric.WMD_sim: 1>, <SimilarityMetric.COS_sim: 2>, <SimilarityMetric.SCM_sim: 3>, <SimilarityMetric.EUC_sim: 4>, <SimilarityMetric.MAN_sim: 5>, <SimilarityMetric.Pearson: 6>] [<LinkType.req2tc: 1>, <LinkType.req2src: 2>]\n"
     ]
    }
   ],
   "source": [
    "print(list(VectorizationType), list(DistanceMetric), list(SimilarityMetric), list(LinkType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSequenceVectorization():\n",
    "    '''Implementation of the class sequence-vanilla-vectorization other classes can inheritance this one'''\n",
    "    def __init__(self, params):\n",
    "                \n",
    "        self.df_source = pd.read_csv(params['source_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        self.df_target = pd.read_csv(params['target_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        self.df_all_system = pd.read_csv(params['system_path'], names=['ids', 'text'], \n",
    "                                         header=0, index_col=0, sep=',')\n",
    "        self.params = params\n",
    "        self.df_nonground_link = None\n",
    "        self.df_ground_link = None\n",
    "        \n",
    "        self.documents = [doc.split() for doc in self.df_all_system['text'].values] #Preparing Corpus\n",
    "        self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "        \n",
    "        \n",
    "        #This can be extended for future metrics <---------------------\n",
    "        self.dict_labels = {\n",
    "            DistanceMetric.COS:[DistanceMetric.COS, SimilarityMetric.COS_sim],\n",
    "            SimilarityMetric.Pearson:[SimilarityMetric.Pearson],\n",
    "            DistanceMetric.EUC:[DistanceMetric.EUC, SimilarityMetric.EUC_sim],\n",
    "            DistanceMetric.WMD:[DistanceMetric.WMD, SimilarityMetric.WMD_sim],\n",
    "            DistanceMetric.SCM:[DistanceMetric.SCM, SimilarityMetric.SCM_sim],\n",
    "            DistanceMetric.MAN:[DistanceMetric.MAN, SimilarityMetric.MAN_sim]\n",
    "        }\n",
    "\n",
    "        \n",
    "    def ground_truth_processing(self, path_to_ground_truth):\n",
    "        'Optional class when corpus has ground truth'\n",
    "        ground_truth = open(path_to_ground_truth,'r')\n",
    "        #Organizing The Ground Truth under the given format\n",
    "        ground_links = [ [(line.strip().split()[0], elem) for elem in line.strip().split()[1:]] for line in ground_truth]\n",
    "        ground_links = functools.reduce(lambda a,b : a+b,ground_links) #reducing into one list\n",
    "        assert len(ground_links) ==  len(set(ground_links)) #To Verify Redundancies in the file\n",
    "        return ground_links\n",
    "    \n",
    "    def samplingLinks(self, sampling = False, samples = 10):\n",
    "        source = [os.path.basename(elem) for elem in self.df_source['ids'].values ] \n",
    "        target = [os.path.basename(elem) for elem in self.df_target['ids'].values ]\n",
    "\n",
    "        if sampling:\n",
    "            links = sample( list( product( source , target ) ), samples)\n",
    "        else:\n",
    "            links = list( product( source , target ))\n",
    "\n",
    "        return links\n",
    "    \n",
    "    def cos_scipy(self, vector_v, vector_w):\n",
    "        cos =  distance.cosine( vector_v, vector_w )\n",
    "        return [cos, 1.-cos]\n",
    "    \n",
    "    def euclidean_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.euclidean(vector_v,vector_w)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def manhattan_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.cityblock(vector_v,vector_w)\n",
    "        n = len(vector_v)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def pearson_abs_scipy(self, vector_v, vector_w):\n",
    "        '''We are not sure that pearson correlation works well on doc2vec inference vectors'''\n",
    "        corr, _ = pearsonr(x, y)\n",
    "        return [abs(corr)] #Absolute value of the correlation\n",
    "    \n",
    "\n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''Metric List Iteration''' \n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "    \n",
    "    def ComputeDistanceArtifacts(self, metric_list, sampling = False , samples = 10):\n",
    "        '''Acticates Distance and Similarity Computations\n",
    "        @metric_list if [] then Computes All metrics\n",
    "        @sampling is False by the default\n",
    "        @samples is the number of samples (or links) to be generated'''\n",
    "        links_ = self.samplingLinks( sampling, samples )\n",
    "        \n",
    "        docs, metric_labels = self.computeDistanceMetric( metric_list=metric_list, links=links_) #checkpoints\n",
    "        self.df_nonground_link = pd.DataFrame(docs, columns =[self.params['names'][0], self.params['names'][1]]+ metric_labels) #Transforming into a Pandas\n",
    "        logging.info(\"Non-groundtruth links computed\")\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def SaveLinks(self, grtruth=False, sep=' ', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system'] + '-' + str(self.params['vectorizationType']) + '-' + str(self.params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "        \n",
    "        if grtruth:\n",
    "            self.df_ground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        else:\n",
    "            self.df_nonground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def findDistInDF(self, g_tuple):\n",
    "        dist = self.df_ground_link[self.df_ground_link[self.params['names'][0]].str.contains( g_tuple[0][:g_tuple[0].find('.')] + '-' ) \n",
    "                     & self.df_ground_link[self.params['names'][1]].str.contains(g_tuple[1][:g_tuple[1].find('.')]) ]        \n",
    "        return dist.index.values\n",
    "        \n",
    "    def MatchWithGroundTruth(self, path_to_ground_truth ):\n",
    "        self.df_ground_link = self.df_nonground_link.copy()\n",
    "        \n",
    "        matchGT = [ self.findDistInDF( g ) for g in self.ground_truth_processing(path_to_ground_truth)]\n",
    "        matchGT = functools.reduce(lambda a,b : np.concatenate([a,b]), matchGT)\n",
    "        \n",
    "        self.df_ground_link[self.params['names'][2]] = 0\n",
    "        new_column = pd.Series(np.full([len(matchGT)], 1 ), name=self.params['names'][2], index = matchGT)\n",
    "        self.df_ground_link.update(new_column)\n",
    "        logging.info(\"Groundtruth links computed\")\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec =  BasicSequenceVectorization(params = parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSeqVect(BasicSequenceVectorization):       \n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Word2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        #Computes cosine similarities between word embeddings and retrieves the closest \n",
    "        #word embeddings by cosine similarity for a given word embedding.\n",
    "        self.similarity_index = WordEmbeddingSimilarityIndex(self.new_model.wv)\n",
    "        #Build a term similarity matrix and compute the Soft Cosine Measure.\n",
    "        self.similarity_matrix = SparseTermSimilarityMatrix(self.similarity_index, self.dictionary)\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.WMD: self.wmd_gensim,\n",
    "            DistanceMetric.SCM: self.scm_gensim\n",
    "        }\n",
    "    \n",
    "    def wmd_gensim(self, sentence_a, sentence_b ):\n",
    "        wmd = self.new_model.wv.wmdistance(sentence_a, sentence_b)\n",
    "        return [wmd, self.wmd_similarity(wmd)]\n",
    "    \n",
    "    def wmd_similarity(self, dist):\n",
    "        return 1./( 1.+float( dist ) ) #Associated Similarity\n",
    "    \n",
    "    def scm_gensim(self, sentence_a, sentence_b ):\n",
    "        '''Compute SoftCosine Similarity of Gensim'''\n",
    "        #Convert the sentences into bag-of-words vectors.\n",
    "        sentence_1 = self.dictionary.doc2bow(sentence_a)\n",
    "        sentence_2 = self.dictionary.doc2bow(sentence_b)\n",
    "        \n",
    "        #Return the inner product(s) between real vectors / corpora vec1 and vec2 expressed in a non-orthogonal normalized basis,\n",
    "        #where the dot product between the basis vectors is given by the sparse term similarity matrix.\n",
    "        scm_similarity = self.similarity_matrix.inner_product(sentence_1, sentence_2, normalized=True)\n",
    "        return [1-scm_similarity, scm_similarity]\n",
    "    \n",
    "    def distance(self, metric_list,link):\n",
    "        '''Iterate on the metrics'''\n",
    "        #Computation of sentences can be moved directly to wmd_gensim method if we cannot generalize it for \n",
    "        #the remaining metrics\n",
    "        sentence_a = self.df_source[self.df_source['ids'].str.contains(link[0])]['text'].values[0].split()\n",
    "        sentence_b = self.df_target[self.df_target['ids'].str.contains(link[1])]['text'].values[0].split()\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](sentence_a,sentence_b) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadLinks(timestamp, params, grtruth=False, sep=' ' ):\n",
    "    '''Returns a pandas from a saved link computation at a give timestamp\n",
    "    @timestamp is the version of the model for a given system'''\n",
    "    \n",
    "    path= params['saving_path'] + '['+ params['system'] + '-' + str(params['vectorizationType']) + '-' + str(params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "    \n",
    "    logging.info(\"Loading computed links from... \"+ path)\n",
    "\n",
    "    return pd.read_csv(path, header=0, index_col=0, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metric1', 'metric2', 'metric1', 'metric2']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_list = ['a','b']\n",
    "A = [[1,3,4],[4,5],[1,8,9,7]]\n",
    "B = ((1,3,4),(4,5),(1,8,9,7))\n",
    "functools.reduce(lambda a,b : a+b, B)\n",
    "dist_sim_T = [([12,13],['metric1','metric2']),([12,13],['metric1','metric2'])]\n",
    "dist_sim_T\n",
    "separated_merged_list_a = functools.reduce(lambda a,b : a[1]+b[1], dist_sim_T)\n",
    "separated_merged_list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data/models/word2vec_libest.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-46a2e5ff253c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecSeqVect\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-6e7af5c67cec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path_to_trained_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalizes the vectors in the word2vec class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#Computes cosine similarities between word embeddings and retrieves the closest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \"\"\"\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \"\"\"\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \"\"\"\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \"\"\"\n\u001b[0;32m-> 1395\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data/models/word2vec_libest.model'"
     ]
    }
   ],
   "source": [
    "word2vec = Word2VecSeqVect( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
